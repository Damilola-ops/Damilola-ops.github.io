<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Home</title>
    <link>//localhost:1313/posts/</link>
    <description>Recent content in Posts on Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright Damilola John &amp;copy; 2023</copyright>
    <lastBuildDate>Thu, 15 Feb 2024 04:14:46 +0100</lastBuildDate><atom:link href="//localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Finetuning GPT2 to Reconstruct Sentences</title>
      <link>//localhost:1313/posts/text-desrambler/</link>
      <pubDate>Thu, 15 Feb 2024 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/posts/text-desrambler/</guid>
      <description>In this article, I would be fine-tuning OPENAI&amp;rsquo;s GPT2 on a non-trivial task of turning descrambled sentences into their grammatically correct forms using the same words in the sentence. It also provides a walkthrough on how to finetune models in modal&amp;rsquo;s gpu cloud and explores different training, optimization and generation strategies.
Through this project, I aimed to explore the capabilities of large language models in performing text reconstruction - a valuable skill with applications in areas like language learning, content moderation, and semantic search.</description>
    </item>
    
    <item>
      <title>The untold problems that arise when Training BPE Tokenizers on large datasets</title>
      <link>//localhost:1313/posts/training-tokenizers/</link>
      <pubDate>Thu, 15 Feb 2024 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/posts/training-tokenizers/</guid>
      <description>The first step to training a language model on a new domain is to train a tokenizer. In this article, we would be focusing on the challenges that arise when training tokenizers on large datasets, in particular, Byte-Pair-Encoding Tokenizers.
If you weren&amp;rsquo;t already familiar with Byte-Pair-Encoding, You can check out Karpathy&amp;rsquo;s(link to karpathy BPE) video or an article I wrote a while agoðŸ‘€
Training a BPE tokenizer is a CPU and memory heavy tasks that can get out of hand pretty quickly.</description>
    </item>
    
    <item>
      <title>Teaching BERT to identify 15 programming languages. </title>
      <link>//localhost:1313/posts/language_classification/</link>
      <pubDate>Sat, 19 Aug 2023 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/posts/language_classification/</guid>
      <description>This is a fun side project where I tried to build a model that could identify 15 of the most popular programming languages. We would start with simple machine learning approaches and gradually work our way up to more complex methods till we have a satisfactory solution.
The Dataset Our dataset is a csv containing 45,000 samples. The dataset is made up of two columns, the &amp;lsquo;code&amp;rsquo; feature contains code snippets we want to classify and the language column, which is our label contains the programming language it belongs to.</description>
    </item>
    
    <item>
      <title> Image Localization and Object Distance Prediction withÂ Pytorch.</title>
      <link>//localhost:1313/posts/image-localization/</link>
      <pubDate>Wed, 16 Aug 2023 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/posts/image-localization/</guid>
      <description>Docking to the ISS would have been more fun if we actually had to dock with the international space station, regardless, we have an interesting problem of trying to create an automatic ISS docker by estimating the coordinates and distance of the international space station from images.
In this articleÂ , we would be using deep learning to estimate the coordinates of an object in an image and also training the model to predict the distance of imagesÂ .</description>
    </item>
    
    <item>
      <title>Learning to smell from Molecules </title>
      <link>//localhost:1313/posts/learning_to_smell/</link>
      <pubDate>Tue, 15 Aug 2023 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/posts/learning_to_smell/</guid>
      <description>The title of this post might be a little confusing but what we are really going to do is to try and build a model to classify smells from their molecular compounds. This little project intertwines the realms of chemistry and machine learning,
Our noses have more than 400 types of olfactory receptors expressed in 1 million+ olfactory sensory neurons, which are all on a small tissue - olfactory epithelium.</description>
    </item>
    
    <item>
      <title>A guide on how AI is changing Computational Photography </title>
      <link>//localhost:1313/posts/computational-photography/</link>
      <pubDate>Thu, 10 Aug 2023 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/posts/computational-photography/</guid>
      <description>And Enhance!! (from Blade Runner ), that&amp;rsquo;s Computational PhotographyÂ .
Computational photography describes signal processing techniques and algorithms that allow computers to replicate photographic processes like motionâ€Š-â€Šblur correctionÂ , auto-focusÂ ,depth-sensingÂ , zoom and other features that would otherwise be impossible without opticsÂ ,while some of these processes use artificial intelligence techniques, Computational Photography is more than just AIÂ , it involves a series of process like that takes an image from the Ones and Zeros on captured by image signal sensors and process to the final image displayed on screensÂ .</description>
    </item>
    
    <item>
      <title>Byte-Pair Encoding, The Tokenization algorithm powering Large LanguageÂ Models. </title>
      <link>//localhost:1313/posts/bpe/</link>
      <pubDate>Thu, 20 Jul 2023 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/posts/bpe/</guid>
      <description>Tokenization is an umbrella term for the methods used to turn texts into chunks of words or sub-words. Tokenization has a lot of applications in computer science, from compilers to Natural Language Processing. In this article, we would be focusing on tokenizers in Language models, in particular, a method of tokenization called Byte Pair Encoding. The last few years have witnessed a revolution in NLP catalyzed mainly by the introduction of the transformers architecture in 2017 with the paper &amp;lsquo;Attention is all you need &amp;rsquo; epitomized by the introduction of ChatGPT in late 2022.</description>
    </item>
    
  </channel>
</rss>
