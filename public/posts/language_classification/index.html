<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Teaching BERT to identify 15 programming languages.  | Home</title>
<meta name="keywords" content="Natural Language Processing, Deep Learning, Transformers">
<meta name="description" content="This is a fun side project where I tried to build a model that could identify 15 of the most popular programming languages. We would start with simple machine learning approaches and gradually work our way up to more complex methods till we have a satisfactory solution.
The Dataset Our dataset is a csv containing 45,000 samples. The dataset is made up of two columns, the &lsquo;code&rsquo; feature contains code snippets we want to classify and the language column, which is our label contains the programming language it belongs to.">
<meta name="author" content="Damilola John">
<link rel="canonical" href="//localhost:1313/posts/language_classification/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bf5f9f73cf17311d52cedbcda82c922e91b2f566d88a85ad9f5b5a08b586bd5f.css" integrity="sha256-v1&#43;fc88XMR1SztvNqCySLpGy9WbYioWtn1taCLWGvV8=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js" integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="//localhost:1313/static/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="//localhost:1313/static/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="//localhost:1313/static/favicon-32x32.png">
<link rel="apple-touch-icon" href="//localhost:1313/static/apple-touch-icon.png">
<link rel="mask-icon" href="//localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="//localhost:1313/posts/language_classification/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Teaching BERT to identify 15 programming languages. " />
<meta property="og:description" content="This is a fun side project where I tried to build a model that could identify 15 of the most popular programming languages. We would start with simple machine learning approaches and gradually work our way up to more complex methods till we have a satisfactory solution.
The Dataset Our dataset is a csv containing 45,000 samples. The dataset is made up of two columns, the &lsquo;code&rsquo; feature contains code snippets we want to classify and the language column, which is our label contains the programming language it belongs to." />
<meta property="og:type" content="article" />
<meta property="og:url" content="//localhost:1313/posts/language_classification/" />
<meta property="og:image" content="//localhost:1313/prog_class.jpg" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-19T04:14:46+01:00" />
<meta property="article:modified_time" content="2023-08-19T04:14:46+01:00" /><meta property="og:site_name" content="Damilola John" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="//localhost:1313/prog_class.jpg" />
<meta name="twitter:title" content="Teaching BERT to identify 15 programming languages. "/>
<meta name="twitter:description" content="This is a fun side project where I tried to build a model that could identify 15 of the most popular programming languages. We would start with simple machine learning approaches and gradually work our way up to more complex methods till we have a satisfactory solution.
The Dataset Our dataset is a csv containing 45,000 samples. The dataset is made up of two columns, the &lsquo;code&rsquo; feature contains code snippets we want to classify and the language column, which is our label contains the programming language it belongs to."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "//localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Teaching BERT to identify 15 programming languages. ",
      "item": "//localhost:1313/posts/language_classification/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Teaching BERT to identify 15 programming languages. ",
  "name": "Teaching BERT to identify 15 programming languages. ",
  "description": "This is a fun side project where I tried to build a model that could identify 15 of the most popular programming languages. We would start with simple machine learning approaches and gradually work our way up to more complex methods till we have a satisfactory solution.\nThe Dataset Our dataset is a csv containing 45,000 samples. The dataset is made up of two columns, the \u0026lsquo;code\u0026rsquo; feature contains code snippets we want to classify and the language column, which is our label contains the programming language it belongs to.",
  "keywords": [
    "Natural Language Processing", "Deep Learning", "Transformers"
  ],
  "articleBody": " This is a fun side project where I tried to build a model that could identify 15 of the most popular programming languages. We would start with simple machine learning approaches and gradually work our way up to more complex methods till we have a satisfactory solution.\nThe Dataset Our dataset is a csv containing 45,000 samples. The dataset is made up of two columns, the ‘code’ feature contains code snippets we want to classify and the language column, which is our label contains the programming language it belongs to.Our train and test datasets were created from stratified sampling based on the target variable.\nExploring the dataset For a clearer picture of our dataset, let’s take a look at the distribution of classes in the dataset We also check the number of unique categories in our label .\nData Cleaning Initially, I started but trying to create a baseline perfomance with no data cleaning or preprocessing.Since we are trying to learn the nuances of different programming languages , I tried to keep the code just as is and see how the models performed before moving forward with any form of preprocessing or feature engineering.\nCreating a Baseline solution Our first model would be a multinomial Naive Bayes classifier. For preprocessing our text, we would try a count vectorizer and tf-idftransformer. We would use the sklearn library’s implementation of the aforementioned algorithms. A bit better than random guesses even when you factor in our heavy class imbalance.\nFinetuning BERT In the spirit of progressively increasing complexity, I have decided to jump the gun and just skip to the state of the art . We would be using the BERT-base model with a classification head(a fully connected layer with pooling applied) to try and solve the problem .\nIn the first training run , I decided to finetune BERT for only 5 epochs , with a max_token_length of 512 and using 16-bit floating point numbers for the model’s weights . As expected, the BERT model perfomance was significantly better than the previous two models we tried with an accuracy of 90% and an F1 score of 0.89 . Great, but we still not good enough . An obervation was I made when I tried handcrafted code samples was that the model was very good at recognizing python and javascript code, but struggled with ‘R’ and Scala. This is explainable by the fact that our training dataset consists of only 127 examples of R and 270 examples of Scala, the model had probably not seen enough R or Scala during training .\nDuring the final run, I trained for 10 epochs using the same training parameters as before and saw a ‘’% accuracy and an F1 score of 90\nGetting better perfomance and model optimization I started to think about ways to improve the model’s perfomance by preprocessing without losing too much useful information.I decided to look at some of the tokenizers outputs when I found out that the BERT BPE tokenizer doesn’t have a token for represent ‘/n’ and ‘/t’, newline and tab characters, respectively. This meant that our model only saw an [UNK] token, which results in a lot of lost information as key programming syntax such as loops and conditionals are defined by both characters. As a workaround, I created new tokens in the tokenizer called [NEWLINE] and [TAB]. I also replaced replaced all instances of integers and floats in the code samples as those are useless anyway and replaced them with [FLOAT] or [INT].\nTraining on the new dataset gave an improved accuracy 92% and F1 score of 0.92 with a smaller DISTILBERT model (BERT but with model distillation) An Interesting side note An interesting problem arises when we try to read our data and tokenize. Since our dataset consists of code snippets that were crawled from the internet, some rows of our dataset contain buggy lines such as unclosed curly brackets for example. The problem with this is that when pandas or any csv parser tries to parse the strings of our dataset and runs into an unexpected EOF character such as an unclosed curly bracket or quotation , since csv parsers rely on balanced structures, unclosed quotations will break the parsing context and cause the parser to raise an EOF error . To work around this , I decided to replace all EOF characters (\"/x1A\") in ASCII as part of the preprocessing and tested the model predictions to see if valuable signals or information where not lost. Another workaround is to use the argument error_bad_lines=False when reading the dataset\nTesting out the model After evaluating the model on an holdout set , our both metrics were still holding good. I decided to try out some code samples suggested by CHATGPT And on python code I wrote myself, I noticed the model confused Rust and C++ code . This could be explained by the fact that Rust and C++ have very similar syntax and are difficult to tell apart even for the average human.\n",
  "wordCount" : "836",
  "inLanguage": "en",
  "image":"//localhost:1313/prog_class.jpg","datePublished": "2023-08-19T04:14:46+01:00",
  "dateModified": "2023-08-19T04:14:46+01:00",
  "author":{
    "@type": "Person",
    "name": "Damilola John"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "//localhost:1313/posts/language_classification/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Home",
    "logo": {
      "@type": "ImageObject",
      "url": "//localhost:1313/static/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="//localhost:1313/" accesskey="h" title="Home (Alt + H)">Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="//localhost:1313/about" title="About Me">
                    <span>About Me</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/articles/" title="Articles">
                    <span>Articles</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="//localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="//localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title">
      Teaching BERT to identify 15 programming languages. 
    </h1>
    <div class="post-meta">&lt;span title=&#39;2023-08-19 04:14:46 &#43;0100 WAT&#39;&gt;August 19, 2023&lt;/span&gt;&amp;nbsp;·&amp;nbsp;4 min&amp;nbsp;·&amp;nbsp;836 words&amp;nbsp;·&amp;nbsp;Damilola John

</div>
  </header> 
<figure class="entry-cover"><img loading="lazy" src="//localhost:1313/prog_class.jpg" alt="">
        
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#the-dataset" aria-label="The Dataset">The Dataset</a></li>
                <li>
                    <a href="#exploring-the-dataset" aria-label="Exploring the dataset">Exploring the dataset</a></li>
                <li>
                    <a href="#data-cleaning" aria-label="Data Cleaning">Data Cleaning</a></li>
                <li>
                    <a href="#creating-a-baseline-solution" aria-label="Creating a Baseline solution">Creating a Baseline solution</a></li>
                <li>
                    <a href="#finetuning-bert" aria-label="Finetuning BERT">Finetuning BERT</a></li>
                <li>
                    <a href="#getting-better-perfomance-and-model-optimization" aria-label="Getting better perfomance and model optimization">Getting better perfomance and model optimization</a></li>
                <li>
                    <a href="#an-interesting-side-note" aria-label="An Interesting side note">An Interesting side note</a></li>
                <li>
                    <a href="#testing-out-the-model" aria-label="Testing out the model">Testing out the model</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="heading"><a hidden class="anchor" aria-hidden="true" href="#heading">#</a></h1>
<p>This is  a fun side project where I tried  to build a model that could identify 15 of the most popular programming languages.
We would start with simple machine learning approaches and gradually work our way up to more complex methods till we have a satisfactory solution.</p>
<h2 id="the-dataset">The Dataset<a hidden class="anchor" aria-hidden="true" href="#the-dataset">#</a></h2>
<p>Our dataset is a csv containing 45,000 samples. The dataset is made up of two columns, the &lsquo;code&rsquo; feature contains  code snippets we want to classify and the language column, which is our label contains the programming language it belongs to.Our train and test datasets were created from stratified sampling based on the target variable.</p>
<h2 id="exploring-the-dataset">Exploring the dataset<a hidden class="anchor" aria-hidden="true" href="#exploring-the-dataset">#</a></h2>
<p>For a clearer picture of our dataset, let&rsquo;s take a look at the distribution of classes in the dataset
<img loading="lazy" src="https://proglangclassifier.s3.eu-west-2.amazonaws.com/class&#43;distribution.png" alt="class distribution"  />
</p>
<p>We also check the number of unique categories in our label .</p>
<h2 id="data-cleaning">Data Cleaning<a hidden class="anchor" aria-hidden="true" href="#data-cleaning">#</a></h2>
<p>Initially, I started but trying to create a baseline perfomance with no data cleaning or preprocessing.Since we are trying to learn the nuances of different programming languages , I tried to keep the code just as is and see how the models performed before moving forward with any form of preprocessing or feature engineering.</p>
<h2 id="creating-a-baseline-solution">Creating a Baseline solution<a hidden class="anchor" aria-hidden="true" href="#creating-a-baseline-solution">#</a></h2>
<p>Our first model would be a multinomial Naive Bayes classifier. For preprocessing our text, we would try  a count vectorizer and tf-idftransformer.
We would use the sklearn library&rsquo;s implementation of the aforementioned algorithms.
<img loading="lazy" src="https://proglangclassifier.s3.eu-west-2.amazonaws.com/naive_baiyes.png" alt="naive bayes"  />

A bit better than random guesses even when you factor in our heavy class imbalance.</p>
<h2 id="finetuning-bert">Finetuning BERT<a hidden class="anchor" aria-hidden="true" href="#finetuning-bert">#</a></h2>
<p>In the spirit of progressively increasing complexity, I have decided to jump the gun and just skip to the state of the art . We would be using the BERT-base model with a classification head(a fully connected layer with pooling applied) to try and solve the problem .</p>
<p>In the first training run , I decided to finetune BERT for only 5 epochs , with a max_token_length of 512 and using 16-bit floating point numbers for the model&rsquo;s weights .
<img loading="lazy" src="https://proglangclassifier.s3.eu-west-2.amazonaws.com/old_accuracy_score.png" alt="old accuracy score"  />
</p>
<p>As expected, the BERT model perfomance was significantly better than the previous two models we tried with an accuracy of 90% and an F1 score of 0.89 . Great, but we still not good enough . An obervation was I made when I tried handcrafted code samples was that the model was very good at recognizing python and javascript code, but  struggled with &lsquo;R&rsquo; and Scala. This is explainable by the fact that our training dataset consists of only 127 examples of R and 270 examples of Scala, the model had probably not seen enough R or Scala during training .</p>
<p>During the final run, I trained for 10 epochs using the same training parameters as before and saw a &lsquo;&rsquo;% accuracy and an F1 score of 90</p>
<h2 id="getting-better-perfomance-and-model-optimization">Getting better perfomance and model optimization<a hidden class="anchor" aria-hidden="true" href="#getting-better-perfomance-and-model-optimization">#</a></h2>
<p>I started to think about ways to improve the model&rsquo;s perfomance by preprocessing  without losing too much useful information.I decided to look at some of the tokenizers outputs when I found out that the BERT BPE tokenizer doesn&rsquo;t have a token for represent &lsquo;/n&rsquo; and &lsquo;/t&rsquo;, newline and tab characters, respectively. This meant that our model only saw an [UNK] token, which results in a lot of lost information as key programming syntax such as loops and conditionals are defined by both characters. As a workaround, I created new tokens in the tokenizer called [NEWLINE] and [TAB]. I also replaced replaced all instances of integers and floats in the code samples as those are useless anyway and replaced them with [FLOAT] or [INT].</p>
<p>Training on the new dataset gave an improved accuracy 92% and F1 score of 0.92 with  a smaller DISTILBERT model (BERT but with model distillation)
<img loading="lazy" src="https://proglangclassifier.s3.eu-west-2.amazonaws.com/acc_with_feature_processing.png" alt="distilbert training scores"  />
</p>
<h2 id="an-interesting-side-note">An Interesting side note<a hidden class="anchor" aria-hidden="true" href="#an-interesting-side-note">#</a></h2>
<p>An interesting problem arises when we try to read our data and tokenize. Since our dataset consists of code snippets that were crawled from the internet, some rows of our dataset contain buggy lines such as unclosed curly brackets for example. The problem with this is that when pandas or any csv parser tries to parse the strings of our  dataset and runs into an unexpected EOF character such as an unclosed curly bracket or quotation , since csv parsers rely on balanced structures, unclosed quotations will break the parsing context and cause the parser to raise an EOF error . To work around this , I decided to replace all EOF characters (&quot;/x1A&quot;) in ASCII as part of the preprocessing and tested the model predictions to see if valuable signals or information where not lost. Another workaround is to use the argument <code> error_bad_lines=False</code> when reading the dataset</p>
<h2 id="testing-out-the-model">Testing out the model<a hidden class="anchor" aria-hidden="true" href="#testing-out-the-model">#</a></h2>
<p>After evaluating the model on an holdout set , our both metrics were still holding good. I decided to try out some code samples suggested by CHATGPT
<img loading="lazy" src="https://proglangclassifier.s3.eu-west-2.amazonaws.com/code_examples.png" alt="code samples image"  />

<img loading="lazy" src="" alt="result"  />

And on python code I wrote myself,
<img loading="lazy" src="https://proglangclassifier.s3.eu-west-2.amazonaws.com/handcrafted_example.png" alt="handcrafted sample"  />
</p>
<p>I noticed the model confused  Rust and C++ code . This could be explained by the fact that Rust and C++ have very similar syntax and are difficult to tell apart even for the average human.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="//localhost:1313/tags/natural-language-processing/">Natural Language Processing</a></li>
      <li><a href="//localhost:1313/tags/deep-learning/">Deep Learning</a></li>
      <li><a href="//localhost:1313/tags/transformers/">Transformers</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="//localhost:1313/posts/training-tokenizers/">
    <span class="title">« Prev</span>
    <br>
    <span>The untold problems that arise when Training BPE Tokenizers on large datasets</span>
  </a>
  <a class="next" href="//localhost:1313/posts/image-localization/">
    <span class="title">Next »</span>
    <br>
    <span> Image Localization and Object Distance Prediction with Pytorch.</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>Copyright Damilola John © 2023</span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
