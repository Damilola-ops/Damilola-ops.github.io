<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>I Tried to teach BERT 15 programming languages.  | Home</title>
<meta name="keywords" content="Natural Language Processing, Deep Learning, Transformers">
<meta name="description" content="This is was a fun side project where I decided to build a model that could identify 15 of the most popular programming languages. We would start with simple machine learning approaches and gradually work our way up to more complex methods till we have a satisfactory solution.
The Dataset Our dataset is a csv containing 45,000 samples . The dataset is made up of two columns, the &lsquo;code&rsquo; feature contains code snippets we want to classify and the language column, which is our label contains the programming language it belongs to.">
<meta name="author" content="Damilola John">
<link rel="canonical" href="damilojohn.github.io/posts/language_classification/">
<link crossorigin="anonymous" href="/damilojohn.github.io/assets/css/stylesheet.bf5f9f73cf17311d52cedbcda82c922e91b2f566d88a85ad9f5b5a08b586bd5f.css" integrity="sha256-v1&#43;fc88XMR1SztvNqCySLpGy9WbYioWtn1taCLWGvV8=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/damilojohn.github.io/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js" integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="/static/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="/static/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="/static/favicon-32x32.png">
<link rel="apple-touch-icon" href="/static/apple-touch-icon.png">
<link rel="mask-icon" href="damilojohn.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="I Tried to teach BERT 15 programming languages. " />
<meta property="og:description" content="This is was a fun side project where I decided to build a model that could identify 15 of the most popular programming languages. We would start with simple machine learning approaches and gradually work our way up to more complex methods till we have a satisfactory solution.
The Dataset Our dataset is a csv containing 45,000 samples . The dataset is made up of two columns, the &lsquo;code&rsquo; feature contains code snippets we want to classify and the language column, which is our label contains the programming language it belongs to." />
<meta property="og:type" content="article" />
<meta property="og:url" content="damilojohn.github.io/posts/language_classification/" />
<meta property="og:image" content="damilojohn.github.io/prog_class.jpg" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-19T04:14:46+01:00" />
<meta property="article:modified_time" content="2023-08-19T04:14:46+01:00" /><meta property="og:site_name" content="Damilola John" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="damilojohn.github.io/prog_class.jpg" />
<meta name="twitter:title" content="I Tried to teach BERT 15 programming languages. "/>
<meta name="twitter:description" content="This is was a fun side project where I decided to build a model that could identify 15 of the most popular programming languages. We would start with simple machine learning approaches and gradually work our way up to more complex methods till we have a satisfactory solution.
The Dataset Our dataset is a csv containing 45,000 samples . The dataset is made up of two columns, the &lsquo;code&rsquo; feature contains code snippets we want to classify and the language column, which is our label contains the programming language it belongs to."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "damilojohn.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "I Tried to teach BERT 15 programming languages. ",
      "item": "damilojohn.github.io/posts/language_classification/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "I Tried to teach BERT 15 programming languages. ",
  "name": "I Tried to teach BERT 15 programming languages. ",
  "description": "This is was a fun side project where I decided to build a model that could identify 15 of the most popular programming languages. We would start with simple machine learning approaches and gradually work our way up to more complex methods till we have a satisfactory solution.\nThe Dataset Our dataset is a csv containing 45,000 samples . The dataset is made up of two columns, the \u0026lsquo;code\u0026rsquo; feature contains code snippets we want to classify and the language column, which is our label contains the programming language it belongs to.",
  "keywords": [
    "Natural Language Processing", "Deep Learning", "Transformers"
  ],
  "articleBody": " This is was a fun side project where I decided to build a model that could identify 15 of the most popular programming languages. We would start with simple machine learning approaches and gradually work our way up to more complex methods till we have a satisfactory solution.\nThe Dataset Our dataset is a csv containing 45,000 samples . The dataset is made up of two columns, the ‘code’ feature contains code snippets we want to classify and the language column, which is our label contains the programming language it belongs to.Our train and test datasets were created from stratified sampling based on the target variable.\nExploring the dataset To get a better picture of our dataset , we look at the distribution of classes in the dataset\nWe also check the number of unique categories in our label .\nData Cleaning We don’t have to do too much data cleaning, the nature of our problem suggests we keep our input just the same , since we are trying to learn the syntax of different languages, it is preferrable to keep the code snippets untouched and hope our model picks up hidden nuances and signals.\nCreating a Baseline solution Our first model would be a multinomial Naive Bayes classifier. For preprocessing our text, we would try a count vectorizer and tf-idftransformer. We would use the sklearn library’s implementation of the aforementioned algorithms.\nUsing BERT as a feature extractor Our first encounter with transformers would be to use them as feature extractors.Using BERT as a feature extractor means using the model hidden states produced in the last layer as features that would be used to train a classifier. The hidden states are simply context-enriched embeddings (a 768-dimensional tensor) produced by multiple self=attention layers in BERT. We would then use this hidden states to train a simple classifier like a Logisitic-Regressor or a Random-Forest. I decided to go with a Logistic Regressor. After fitting the model, to our hidden states and trying to make predictions , our model performs better than the naive baiyes classifier with an accuracy of ’ ‘, a step in the right direction but definitely not where we would like to be if we want a model that knows how to classify languages\nFinetuning BERT In the spirit of progressively increasing complexity, I have decided to jump the gun and just skip to the state of the art . We would be using the BERT-base model with a classification head(a fully connected layer with pooling applied) to try and solve the problem .\nIn the first training run , I decided to finetune BERT for only 5 epochs , with a max_token_length of 512 and using 16-bit floating point numbers for the model’s weights .\nAs expected, the BERT model perfomance was significantly better than the previous two models we tried with an accuracy of 90% and an F1 score of 89 . Great, but we still not good enough . An obervation was I made when I tried handcrafted code samples was that the model was very good at recognizing python and javascript code, but struggled with ‘R’ and Scala. This is explainable by the fact that our training dataset consists of only 127 examples of R and 270 examples of Scala, the model had probably not seen enough R or Scala during training .\nDuring the final run, I trained for 10 epochs using the same training parameters as before and saw a ‘’% accuracy and an F1 score of 90\nAn Intersting side note An interesting problem arises when we try to read our data and tokenize. Since our dataset consists of code snippets that were crawled from the internet, some rows of our dataset contain buggy lines such as unclosed curly brackets for example. The problem with this is that when pandas or any csv parser tries to parse the strings of our dataset and runs into an unexpected EOF character such as an unclosed curly bracket or quotation , since csv parsers rely on balanced structures, unclosed quotations will break the parsing context and cause the parser to raise an EOF error . To work around this , I decided to replace all EOF characters (\"/x1A\") in ASCII as part of the preprocessing and tested the model predictions to see if valuable signals or information where not lost. Another workaround is to use the argument error_bad_lines=False when reading the dataset\n",
  "wordCount" : "737",
  "inLanguage": "en",
  "image":"damilojohn.github.io/prog_class.jpg","datePublished": "2023-08-19T04:14:46+01:00",
  "dateModified": "2023-08-19T04:14:46+01:00",
  "author":{
    "@type": "Person",
    "name": "Damilola John"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "damilojohn.github.io/posts/language_classification/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Home",
    "logo": {
      "@type": "ImageObject",
      "url": "/static/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="damilojohn.github.io" accesskey="h" title="Home (Alt + H)">Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="damilojohn.github.io/damilojohn.github.io/about-me" title="About Me">
                    <span>About Me</span>
                </a>
            </li>
            <li>
                <a href="damilojohn.github.io/damilojohn.github.io/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="damilojohn.github.io/damilojohn.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="damilojohn.github.io/damilojohn.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="damilojohn.github.io/damilojohn.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="damilojohn.github.io">Home</a>&nbsp;»&nbsp;<a href="damilojohn.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      I Tried to teach BERT 15 programming languages. 
    </h1>
    <div class="post-meta"><span title='2023-08-19 04:14:46 +0100 WAT'>August 19, 2023</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;737 words&nbsp;·&nbsp;Damilola John

</div>
  </header> 
<figure class="entry-cover"><img loading="lazy" src="damilojohn.github.io/prog_class.jpg" alt="">
        
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul><ul>
                <li>
                    <a href="#the-dataset" aria-label="The Dataset">The Dataset</a></li></ul>
                    
                <li>
                    <a href="#exploring-the-dataset" aria-label="Exploring the dataset">Exploring the dataset</a><ul>
                        
                <li>
                    <a href="#data-cleaning" aria-label="Data Cleaning">Data Cleaning</a></li>
                <li>
                    <a href="#creating-a-baseline-solution" aria-label="Creating a Baseline solution">Creating a Baseline solution</a></li>
                <li>
                    <a href="#using-bert-as-a-feature-extractor" aria-label="Using BERT as a feature extractor">Using BERT as a feature extractor</a></li>
                <li>
                    <a href="#finetuning-bert" aria-label="Finetuning BERT">Finetuning BERT</a></li>
                <li>
                    <a href="#an-intersting-side-note" aria-label="An Intersting side note">An Intersting side note</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="heading"><a hidden class="anchor" aria-hidden="true" href="#heading">#</a></h1>
<p>This is was a fun side project where I decided to build a model that could identify 15 of the most popular programming languages.
We would start with simple machine learning approaches and gradually work our way up to more complex methods till we have a satisfactory solution.</p>
<h2 id="the-dataset">The Dataset<a hidden class="anchor" aria-hidden="true" href="#the-dataset">#</a></h2>
<p>Our dataset is a csv containing 45,000 samples . The dataset is made up of two columns, the &lsquo;code&rsquo; feature contains  code snippets we want to classify and the language column, which is our label contains the programming language it belongs to.Our train and test datasets were created from stratified sampling based on the target variable.</p>
<h1 id="exploring-the-dataset">Exploring the dataset<a hidden class="anchor" aria-hidden="true" href="#exploring-the-dataset">#</a></h1>
<p>To get a better picture of our dataset , we look at the distribution of classes in the dataset</p>
<p>We also check the number of unique categories in our label .</p>
<h2 id="data-cleaning">Data Cleaning<a hidden class="anchor" aria-hidden="true" href="#data-cleaning">#</a></h2>
<p>We don&rsquo;t have to do too much data cleaning, the nature of our problem suggests we keep our input just the same , since we are trying to learn the syntax of different languages, it is preferrable to keep the code snippets untouched and hope our model picks up hidden nuances and signals.</p>
<h2 id="creating-a-baseline-solution">Creating a Baseline solution<a hidden class="anchor" aria-hidden="true" href="#creating-a-baseline-solution">#</a></h2>
<p>Our first model would be a multinomial Naive Bayes classifier. For preprocessing our text, we would try  a count vectorizer and tf-idftransformer.
We would use the sklearn library&rsquo;s implementation of the aforementioned algorithms.</p>
<h2 id="using-bert-as-a-feature-extractor">Using BERT as a feature extractor<a hidden class="anchor" aria-hidden="true" href="#using-bert-as-a-feature-extractor">#</a></h2>
<p>Our first encounter with transformers would be to use them as feature extractors.Using BERT as a feature extractor means using the model hidden states produced in the last layer as features that would be used to train a classifier. The hidden states are simply context-enriched embeddings (a 768-dimensional tensor) produced by multiple self=attention layers in BERT. We would then use this hidden states to train a simple classifier like a Logisitic-Regressor or a Random-Forest. I decided to go with a Logistic Regressor. After fitting the model, to our hidden states and trying to make predictions , our model performs better than the naive baiyes classifier with an accuracy of &rsquo; &lsquo;, a step in the right direction but definitely not where we would like to be if we want a model that knows how to classify languages</p>
<h2 id="finetuning-bert">Finetuning BERT<a hidden class="anchor" aria-hidden="true" href="#finetuning-bert">#</a></h2>
<p>In the spirit of progressively increasing complexity, I have decided to jump the gun and just skip to the state of the art . We would be using the BERT-base model with a classification head(a fully connected layer with pooling applied) to try and solve the problem .</p>
<p>In the first training run , I decided to finetune BERT for only 5 epochs , with a max_token_length of 512 and using 16-bit floating point numbers for the model&rsquo;s weights .</p>
<p>As expected, the BERT model perfomance was significantly better than the previous two models we tried with an accuracy of 90% and an F1 score of 89 . Great, but we still not good enough . An obervation was I made when I tried handcrafted code samples was that the model was very good at recognizing python and javascript code, but  struggled with &lsquo;R&rsquo; and Scala. This is explainable by the fact that our training dataset consists of only 127 examples of R and 270 examples of Scala, the model had probably not seen enough R or Scala during training .</p>
<p>During the final run, I trained for 10 epochs using the same training parameters as before and saw a &lsquo;&rsquo;% accuracy and an F1 score of 90</p>
<h2 id="an-intersting-side-note">An Intersting side note<a hidden class="anchor" aria-hidden="true" href="#an-intersting-side-note">#</a></h2>
<p>An interesting problem arises when we try to read our data and tokenize. Since our dataset consists of code snippets that were crawled from the internet, some rows of our dataset contain buggy lines such as unclosed curly brackets for example. The problem with this is that when pandas or any csv parser tries to parse the strings of our  dataset and runs into an unexpected EOF character such as an unclosed curly bracket or quotation , since csv parsers rely on balanced structures, unclosed quotations will break the parsing context and cause the parser to raise an EOF error . To work around this , I decided to replace all EOF characters (&quot;/x1A&quot;) in ASCII as part of the preprocessing and tested the model predictions to see if valuable signals or information where not lost. Another workaround is to use the argument <code> error_bad_lines=False</code> when reading the dataset</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="damilojohn.github.io/tags/natural-language-processing/">Natural Language Processing</a></li>
      <li><a href="damilojohn.github.io/tags/deep-learning/">Deep Learning</a></li>
      <li><a href="damilojohn.github.io/tags/transformers/">Transformers</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="damilojohn.github.io/posts/image-localization/">
    <span class="title">Next »</span>
    <br>
    <span> Image Localization and Object Distance Prediction with Pytorch.</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>Copyright Damilola John © 2023</span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
