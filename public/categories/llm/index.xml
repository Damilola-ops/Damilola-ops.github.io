<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>LLM on Home</title>
    <link>//localhost:1313/categories/llm/</link>
    <description>Recent content in LLM on Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright Damilola John &amp;copy; 2023</copyright>
    <lastBuildDate>Wed, 11 Dec 2024 04:14:46 +0100</lastBuildDate><atom:link href="//localhost:1313/categories/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Understanding Differential Attention Transformers.</title>
      <link>//localhost:1313/articles/diff_attention/</link>
      <pubDate>Wed, 11 Dec 2024 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/articles/diff_attention/</guid>
      <description>Introduction The [Transformers](add reference here) architecture has been the de-facto archictecture for language models for some time now. The unprecendented growth in their applications to real world products such as in the fastest growing software application of all time, ChatGPT, particularly in large language models have seen them become the de-facto architecture.
The transformers architecture owes most of it&amp;rsquo;s ubuquitousness however to it&amp;rsquo;s scaling laws. Transformers, Language models in particular, have demonstrated exponentially increasing abilities across varying tasks, including but not limited to reasoning, translation, language understanding and comprehension, etc, with ever increasing sizes.</description>
    </item>
    
    <item>
      <title>Finetuning GPT2 to Reconstruct Sentences</title>
      <link>//localhost:1313/articles/text-desrambler/</link>
      <pubDate>Sat, 15 Jun 2024 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/articles/text-desrambler/</guid>
      <description>Two words are anagrams if one can be formed by permuting the letters of the other. Applying the same logic to a sentence, would be saying that two sentences are anagrams(no such thing) if their component words can be permutated to form clones of each other.
I thought it would be interesting to teach a language model to do this. You might be thinking that simply re-arranging words in a sentence doesn&amp;rsquo;t require intelligence and can be done with very trivial algorithms,you would be right, but I added an edge to this task, given a random sequence of words, the language model has to return a grammatically correct sequence using the same set of words.</description>
    </item>
    
    <item>
      <title>Descrambling Sentences with GPT2 </title>
      <link>//localhost:1313/projects/text_descrambler/</link>
      <pubDate>Mon, 10 Jun 2024 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/projects/text_descrambler/</guid>
      <description>Finetuning GPT2 to Reconstruct sentences Two words are anagrams if one can be formed by permuting the letters of the other. Applying the same logic to a sentence, would be saying that two sentences are anagrams(no such thing) if their component words can be permutated to form clones of each other.
I thought it would be interesting to finetune a language model to do this. You might be thinking that simply re-arranging words in a sentence doesn&amp;rsquo;t require intelligence and can be done with very trivial algorithms,you would be right, but I added an edge to this task, given a random sequence of words, the language model has to return a grammatically correct sequence using the same set of words.</description>
    </item>
    
    <item>
      <title>SabiYarn: Training a Foundational Language Model on Nigerian Languages</title>
      <link>//localhost:1313/projects/sabiyarn/</link>
      <pubDate>Fri, 15 Mar 2024 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/projects/sabiyarn/</guid>
      <description>Training a 125M parameter decoder-only model on 70b tokens of Nigerian Languages. demo
Article</description>
    </item>
    
    <item>
      <title>Classifying Code snippets with BERT.</title>
      <link>//localhost:1313/articles/language_classification/</link>
      <pubDate>Sat, 19 Aug 2023 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/articles/language_classification/</guid>
      <description>This is a fun side project where I explored transformers based sentiment classification for the first time by training BERT to identify 15 of the most popular programming languages.
i startED with simple machine learning approaches and gradually work our way up to more complex methods till we have a satisfactory solution.
The Dataset Our dataset is a csv containing 45,000 samples. The dataset is made up of two columns, the &amp;lsquo;code&amp;rsquo; feature contains code snippets we want to classify and the language column, which is our label contains the programming language it belongs to.</description>
    </item>
    
  </channel>
</rss>
