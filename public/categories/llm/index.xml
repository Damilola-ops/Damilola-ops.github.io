<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>LLM on Home</title>
    <link>//localhost:1313/categories/llm/</link>
    <description>Recent content in LLM on Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright Damilola John &amp;copy; 2023</copyright>
    <lastBuildDate>Thu, 15 Feb 2024 04:14:46 +0100</lastBuildDate><atom:link href="//localhost:1313/categories/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Finetuning GPT2 to Reconstruct Sentences</title>
      <link>//localhost:1313/articles/text-desrambler/</link>
      <pubDate>Thu, 15 Feb 2024 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/articles/text-desrambler/</guid>
      <description>In this article, I would be fine-tuning OPENAI&amp;rsquo;s GPT2 on a non-trivial task of turning descrambled sentences into their grammatically correct forms using the same words in the sentence. It also provides a walkthrough on how to finetune models in modal&amp;rsquo;s gpu cloud and explores different training, optimization and generation strategies.
Through this project, I aimed to explore the capabilities of large language models in performing text reconstruction - a valuable skill with applications in areas like language learning, content moderation, and semantic search.</description>
    </item>
    
    <item>
      <title>The untold problems that arise when Training BPE Tokenizers on large datasets</title>
      <link>//localhost:1313/articles/training-tokenizers/</link>
      <pubDate>Thu, 15 Feb 2024 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/articles/training-tokenizers/</guid>
      <description>The first step to training a language model on a new domain is to train a tokenizer. In this article, we would be focusing on the challenges that arise when training tokenizers on large datasets, in particular, Byte-Pair-Encoding Tokenizers.
If you weren&amp;rsquo;t already familiar with Byte-Pair-Encoding, You can check out Karpathy&amp;rsquo;s(link to karpathy BPE) video or an article I wrote a while agoðŸ‘€
Training a BPE tokenizer is a CPU and memory heavy tasks that can get out of hand pretty quickly.</description>
    </item>
    
    <item>
      <title>Teaching BERT to identify 15 programming languages. </title>
      <link>//localhost:1313/articles/language_classification/</link>
      <pubDate>Sat, 19 Aug 2023 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/articles/language_classification/</guid>
      <description>This is a fun side project where I tried to build a model that could identify 15 of the most popular programming languages. We would start with simple machine learning approaches and gradually work our way up to more complex methods till we have a satisfactory solution.
The Dataset Our dataset is a csv containing 45,000 samples. The dataset is made up of two columns, the &amp;lsquo;code&amp;rsquo; feature contains code snippets we want to classify and the language column, which is our label contains the programming language it belongs to.</description>
    </item>
    
  </channel>
</rss>
