<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>NLP on Home</title>
    <link>//localhost:1313/categories/nlp/</link>
    <description>Recent content in NLP on Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright Damilola John &amp;copy; 2023</copyright>
    <lastBuildDate>Wed, 15 May 2024 04:14:46 +0100</lastBuildDate><atom:link href="//localhost:1313/categories/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Building A Text based Books Recommendation engine</title>
      <link>//localhost:1313/articles/zeeno/</link>
      <pubDate>Wed, 15 May 2024 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/articles/zeeno/</guid>
      <description>Building a text-based books recommendation engine Find books by describing what they are about.
Finding books What does that really mean? A recommendation engine is an algorithm or bunch of algorithms that help users find contents about a particular modality(video, text etc). A text based recommendation engine is one that helps users find information about some text input.
Semantic Search Architecture Zeeno was built using a combination of semantic search and hybrid keyword search.</description>
    </item>
    
    <item>
      <title>Art History Educational Tool</title>
      <link>//localhost:1313/projects/askarthistory/</link>
      <pubDate>Thu, 15 Feb 2024 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/projects/askarthistory/</guid>
      <description>A telegram bot that answers questions on Modern Art History. Built using LlamaIndex and GPT3.5.</description>
    </item>
    
    <item>
      <title>Finetuning GPT2 to Reconstruct Sentences</title>
      <link>//localhost:1313/articles/text-desrambler/</link>
      <pubDate>Thu, 15 Feb 2024 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/articles/text-desrambler/</guid>
      <description>Two words are anagrams if one can be formed by permuting the letters of the other. Applying the same logic to a sentence, would be saying that two sentences are anagrams(no such thing) if their component words can be permutated to form clones of each other.
I thought it would be interesting to teach a language model to do this. You might be thinking that simply re-arranging words in a sentence doesn&amp;rsquo;t require intelligence and can be done with very trivial algorithms,you would be right, but I added an edge to this task, given a random sequence of words, the language model has to return a grammatically correct sequence using the same words.</description>
    </item>
    
    <item>
      <title>Personalized Prompt Learning for Explainable Recommendations.</title>
      <link>//localhost:1313/projects/pepler/</link>
      <pubDate>Thu, 15 Feb 2024 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/projects/pepler/</guid>
      <description>An Implementation of the PEPLER paper ,an exploration in soft-prompt tuning.</description>
    </item>
    
    <item>
      <title>The untold problems that arise when Training Byte-Pair-Encoding Tokenizers on large datasets</title>
      <link>//localhost:1313/articles/training-tokenizers/</link>
      <pubDate>Thu, 15 Feb 2024 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/articles/training-tokenizers/</guid>
      <description>The first step to training a language model on a new domain is to train a tokenizer. In this article, we would be focusing on the challenges that arise when training tokenizers on large datasets, in particular, Byte-Pair-Encoding Tokenizers.
If you weren&amp;rsquo;t already familiar with Byte-Pair-Encoding, You can check out Karpathy&amp;rsquo;s(link to karpathy BPE) video or an article I wrote a while agoðŸ‘€
Training a BPE tokenizer is a CPU and memory heavy tasks that can get out of hand pretty quickly.</description>
    </item>
    
    <item>
      <title>Learning to smell from Molecules </title>
      <link>//localhost:1313/articles/learning_to_smell/</link>
      <pubDate>Tue, 15 Aug 2023 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/articles/learning_to_smell/</guid>
      <description>The title of this post might be a little confusing but what we are really going to do is to try and build a model to classify smells from their molecular compounds. This little project intertwines the realms of chemistry and machine learning,
Our noses have more than 400 types of olfactory receptors expressed in 1 million+ olfactory sensory neurons, which are all on a small tissue - olfactory epithelium.</description>
    </item>
    
  </channel>
</rss>
