<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>NLP on Home</title>
    <link>//localhost:1313/categories/nlp/</link>
    <description>Recent content in NLP on Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright Damilola John &amp;copy; 2023</copyright>
    <lastBuildDate>Thu, 15 Feb 2024 04:14:46 +0100</lastBuildDate><atom:link href="//localhost:1313/categories/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Art History Educational Tool</title>
      <link>//localhost:1313/projects/askarthistory/</link>
      <pubDate>Thu, 15 Feb 2024 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/projects/askarthistory/</guid>
      <description>A telegram bot that answers questions on Modern Art History. Built using LlamaIndex and GPT3.5.</description>
    </item>
    
    <item>
      <title>Finetuning GPT2 to Reconstruct Sentences</title>
      <link>//localhost:1313/articles/text-desrambler/</link>
      <pubDate>Thu, 15 Feb 2024 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/articles/text-desrambler/</guid>
      <description>In this article, I would be fine-tuning OPENAI&amp;rsquo;s GPT2 on a non-trivial task of turning descrambled sentences into their grammatically correct forms using the same words in the sentence. It also provides a walkthrough on how to finetune models in modal&amp;rsquo;s gpu cloud and explores different training, optimization and generation strategies.
Through this project, I aimed to explore the capabilities of large language models in performing text reconstruction - a valuable skill with applications in areas like language learning, content moderation, and semantic search.</description>
    </item>
    
    <item>
      <title>Personalized Prompt Learning for Explainable Recommendations.</title>
      <link>//localhost:1313/projects/pepler/</link>
      <pubDate>Thu, 15 Feb 2024 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/projects/pepler/</guid>
      <description>An Implementation of the PEPLER paper ,an exploration in soft-prompt tuning.</description>
    </item>
    
    <item>
      <title>The untold problems that arise when Training BPE Tokenizers on large datasets</title>
      <link>//localhost:1313/articles/training-tokenizers/</link>
      <pubDate>Thu, 15 Feb 2024 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/articles/training-tokenizers/</guid>
      <description>The first step to training a language model on a new domain is to train a tokenizer. In this article, we would be focusing on the challenges that arise when training tokenizers on large datasets, in particular, Byte-Pair-Encoding Tokenizers.
If you weren&amp;rsquo;t already familiar with Byte-Pair-Encoding, You can check out Karpathy&amp;rsquo;s(link to karpathy BPE) video or an article I wrote a while agoðŸ‘€
Training a BPE tokenizer is a CPU and memory heavy tasks that can get out of hand pretty quickly.</description>
    </item>
    
    <item>
      <title>Learning to smell from Molecules </title>
      <link>//localhost:1313/articles/learning_to_smell/</link>
      <pubDate>Tue, 15 Aug 2023 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/articles/learning_to_smell/</guid>
      <description>The title of this post might be a little confusing but what we are really going to do is to try and build a model to classify smells from their molecular compounds. This little project intertwines the realms of chemistry and machine learning,
Our noses have more than 400 types of olfactory receptors expressed in 1 million+ olfactory sensory neurons, which are all on a small tissue - olfactory epithelium.</description>
    </item>
    
  </channel>
</rss>
