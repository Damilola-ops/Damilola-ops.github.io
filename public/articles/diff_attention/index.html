<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Understanding Differential Attention. | Home</title>
<meta name="keywords" content="Natural Language Processing, Transformers">
<meta name="description" content="Introduction Over the last few years, Transformers have emerged as the de-facto deep learning architecture in language models. Fundamentally changing the field of machine learning and Artificial intelligence as a whole. Their unprecendented success in solving complex language tasks, reasoning (or mimmicking it) in solving math and coding problems, have ushered in a new era in AI, powering successful AI products like ChatGPT.
The key innovation of transformers lies in the self-attention mechanism, which allows each tokens in the input sequence to directly interact with every other token in the sequence.">
<meta name="author" content="Damilola John">
<link rel="canonical" href="//localhost:1313/articles/diff_attention/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.51ae2dfd23423d479cfd90ba4215aeebadfd8c0833c61a978bfbc8648cc56365.css" integrity="sha256-Ua4t/SNCPUec/ZC6QhWu6639jAgzxhqXi/vIZIzFY2U=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js" integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="//localhost:1313/static/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="//localhost:1313/static/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="//localhost:1313/static/favicon-32x32.png">
<link rel="apple-touch-icon" href="//localhost:1313/static/apple-touch-icon.png">
<link rel="mask-icon" href="//localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="//localhost:1313/articles/diff_attention/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
</head>

<head>
    ...
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '\\[', right: '\\]', display: true},   
        {left: '$$', right: '$$', display: true},     
        {left: '\\(', right: '\\)', display: false},  
      ],
      throwOnError : false
    });
  });
</script>
    
    ...
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="//localhost:1313/" accesskey="h" title="Home (Alt + H)">Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="//localhost:1313/about" title="About Me">
                    <span>About Me</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/articles/" title="Articles">
                    <span>Articles</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="//localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="//localhost:1313/articles/">Articles</a></div>
    <h1 class="post-title">
      Understanding Differential Attention.
    </h1>
    <div class="post-meta">&lt;span title=&#39;2024-12-11 04:14:46 &#43;0100 WAT&#39;&gt;December 11, 2024&lt;/span&gt;&amp;nbsp;·&amp;nbsp;7 min&amp;nbsp;·&amp;nbsp;1424 words&amp;nbsp;·&amp;nbsp;Damilola John

</div>
  </header> 
<figure class="entry-cover"><img loading="lazy" src="//localhost:1313/diff_attn.png" alt="">
        
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#noise-in-self-attention" aria-label="Noise in Self Attention.">Noise in Self Attention.</a></li>
                <li>
                    <a href="#differential-attention-architecture" aria-label="Differential Attention Architecture">Differential Attention Architecture</a></li>
                <li>
                    <a href="#implementation" aria-label="Implementation">Implementation</a></li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Over the last few years, Transformers have emerged as the de-facto deep learning architecture in language models. Fundamentally changing the field of machine learning and Artificial intelligence as a whole.
Their unprecendented success in solving complex language tasks, reasoning (or mimmicking it) in solving math and coding problems, have ushered in a new era in AI, powering successful AI products like ChatGPT.</p>
<p>The key innovation of transformers lies in the self-attention mechanism, which allows each tokens in the input sequence to directly interact with every other token in the sequence.</p>
<p>The self-attention mechanism is a series of transformations that allow transformers introduce information about a token&rsquo;s context into it&rsquo;s latent space representation( aka Embeddings).</p>
<p>In simple terms, self attention turns each token&rsquo;s embeddings into a weighted sum of all the other embeddings in a sequence, creating an embedding that incorporates information about the other tokens in the sequence.</p>
<p><img loading="lazy" src="%27/self_attn.png%27" alt="self-attention example"  />
</p>
<p>The goal of self-attention in the image above would be to create an embedding for the token &ldquo;flies&rdquo; that encodes the flow of time in the first sequence, and one that has encodes flies in relation to insects in the second sequence.</p>
<p>In recent times, most of the spotlight in research on self-attention has been on techniques focused on optimizing computational and memory efficiency such as <img loading="lazy" src="https://arxiv.org/abs/2205.14135" alt="Flash attention (2022)"  />
. However transformers are still notorious</p>
<p><strong>Differential Attention was introduced in a 2024 <a href="https://arxiv.org/pdf/2410.05258">paper</a> by a team at Microsoft called Differential Transformer that proposes a new transformers architecture. However the only major difference between a differential transformer and a transformer is the differential attention mechanism</strong></p>
<h2 id="noise-in-self-attention">Noise in Self Attention.<a hidden class="anchor" aria-hidden="true" href="#noise-in-self-attention">#</a></h2>
<p>As mentioned earlier, the goal of self-attention is to include information about a token&rsquo;s context into it&rsquo;s embeddings. This is carried out  by performing a series of transformations that take the embeddings of all tokens in the input and returns context-aware embeddings that can be thought of as weighted sums of learned representations of all tokens in the input sequence. The weights computed in this process are called attention weights and they represent how much attention is paid to other tokens in the sequence.</p>
<p>Transformers for all of their glory however, are still notorious for paying attention to irrelevant context in a sequence. By visualizing the attention scores of a transformer model during a retrieval task, the authors of differential attention compared the attention maps of transformers to those of differential transformers.</p>
<p>(attention scores visualization)</p>
<p>The key idea behind Differential Attention is a technique called Differential Denoising.</p>
<p>This is a simple process that involves substracting two different attention weights computed from two attention maps. To readers with a background in Electrical Engineering, this might sound familiar to a common denoising electrical component, the Differential Amplifier.</p>
<p>Differential Amplifiers are amplifiers that are mainly used to reduce noise in signals. They amplify the <strong>difference</strong> between two input signals and remove noise in the process. They key idea behind this is that, if noise exists in a system, then equal amount of noise added to the input signals. If A and B are provided as the inputs to a Differential Amplifier, the differential amplifier amplifies the difference between signal A and B, and rejects( provides low gain) their commmon input (the noise) simply called the common mode.</p>
<p>Differential Denoising also finds application in Noise-Cancellation headphones.</p>
<p>Now that the core idea of removing noise by taking the difference between two signals has been explored, let&rsquo;s take a look at the inner workings of a differential amplifier.</p>
<h2 id="differential-attention-architecture">Differential Attention Architecture<a hidden class="anchor" aria-hidden="true" href="#differential-attention-architecture">#</a></h2>
<p>The key difference between Differential attention and self-attention mechanism lies in how their attention weights are computed.</p>
<p>The differential attention mechanism computes outputs using the equation:</p>
\[
\text{DiffAttn}(X) = \left( \text{softmax}\left(\frac{Q_1 K_1^T}{\sqrt{d}}\right) - \lambda \cdot \text{softmax}\left(\frac{Q_2 K_2^T}{\sqrt{d}}\right) \right) V
\]
<p>where:</p>
<ul>
<li>\( Q_1, Q_2 \) are query matrices,</li>
<li>\( K_1, K_2 \) are key matrices,</li>
<li>\( V \) is the value matrix,</li>
<li>\( \lambda \) is a learnable scalar,</li>
<li>\( d \) is the head dimension.</li>
</ul>
<p>As a comparison, here&rsquo;s how self attention computes outputs:</p>
\[
\text{SelfAttn}(X) = \left( \text{softmax}\left(\frac{Q K^T}{\sqrt{d}}
\right) \right)
\]
<h2 id="implementation">Implementation<a hidden class="anchor" aria-hidden="true" href="#implementation">#</a></h2>
<p>This implementation assumes that the differential attention layer is implemented as a standalone module.</p>
<pre tabindex="0"><code>class DiffAttention(nn.Module):
  def __init__(self, args: DiffAttnArgs,):
</code></pre><p>A key detail to pay close attention to is that we set the number of attention heads to half that of a normal transformer model, then proceed to set the dimension of each head to</p>
<pre tabindex="0"><code>self.num_heads = args.n_heads # half of transformers head
self.head_dim = args.dim // args.n_heads // 2
</code></pre><p>This might seem odd, as intuitively, each head usually has a dimension of:</p>
<pre tabindex="0"><code>self.head_dim = args.dim // args.n_heads 
</code></pre><p>Taking GPT-2-small as an example, with 12 attention heads and each head having a head dimension of 64. Using differential attention, we would set the number of heads to 6 instead, and have a head dimension of 64 (with 6 heads), when we really should have a head dimension of 128 with 6 heads(following transformer&rsquo;s convention).</p>
<p>The key, query and vector projection layers are instantiated just like they are in self-attention:</p>
<pre tabindex="0"><code>    self.wq = nn.Linear(args.embed_dim, args.embed_dim, bias=False)
    self.wk = nn.Linear(args.embed_dim, args.embed_dim, bias=False)
    self.wv = nn.Linear(args.embed_dim, args.embed_dim, bias=False)
</code></pre><p>the scaling constant is defined as :</p>
<pre tabindex="0"><code>self.scaling = self.head_dim ** -0.5
</code></pre><p>the same as in self-attention, if you chose to multiply by the numerator (the dot product of key and query vectors).</p>
<p>In order to balance gradient computation with the rest of the model, the lambda value is parameterized as :</p>
\[
\lambda = e^{(\lambda_{q1} \cdot \lambda_{k1})} - e^{(\lambda_{q2} \cdot \lambda_{k2})} + \lambda_{\text{init}}
\]
<p>where
</p>
\[ 
\lambda_{q1} ,\lambda_{k1}, \lambda_{q2}, \lambda_{q2}
\]
<p> are <strong>learnable vectors</strong> and are instantiated as :</p>
<pre tabindex="0"><code>self.lambda_q1 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32))
self.lambda_q2 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32))
self.lambda_k1 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32))
self.lambda_k2 = nn.Parameter(torch.zeros(self.head_dim, dtype=torch.float32))
</code></pre>\[
\lambda_{\text{init}} = 0.8 - 0.6 \times e^{-0.3 \cdot (l - 1)}
\]
<p>
is defined as:</p>
<pre tabindex="0"><code>def lambda_init_fn(depth):
      &#34;&#34;&#34;
      Function for calculating Lambda_init
      Args:
            depth (int): Decoder layer index containing the attention mechanism.
      Returns: 
            float: lambda init value.
      &#34;&#34;&#34;
      
      return 0.8 - 0.6 * math.exp(-0.3 * depth)
</code></pre><p>where L or <strong>depth</strong> is the layer index (index of the decoder layer the attention module resides in).</p>
<p>finally:</p>
<pre tabindex="0"><code>self.lambda_init = lambda_init_fn(args.depth)
</code></pre><p>Layer normalization is also initialized using <a href="https://pytorch.org/docs/stable/generated/torch.nn.modules.normalization.RMSNorm.html">Root-Mean-Square Normalization</a>.</p>
<pre tabindex="0"><code>    self.sublayer_norm = RMSNorm(2 * self.head_dim, eps=1e-5,)
</code></pre><p>The forward method is then implemented as follows:</p>
<ul>
<li>The key, query and value vectors are computed and split into n_heads for multihead attention.
<pre tabindex="0"><code>q = self.wq(x)
k = self.wk(x)
v = self.wv(x)
q = q.view(bsz, tgt_len, 2*self.num_heads, self.head_dim)
k = k.view(bsz, tgt_len, 2*self.num_heads, self.head_dim)
v = v.view(bsz, tgt_len, self.num_heads, 2*self.head_dim)
</code></pre></li>
<li>Next, <a href="https://arxiv.org/abs/2104.09864">Rotary Positional Embeddings</a> are added to the Key and Query vectors to include positional information in the sequence
<pre tabindex="0"><code>q,k = apply_rotary_emb(q, k, freqs_cis)
</code></pre></li>
</ul>
<p>To make up for the halved head_dim, the key and query heads each have twice the number of heads (head_dim should always equal embed_dim / num_heads).
The same is done in the value vector however, here we use 2x the head dimension.</p>
<p>Having twice the number of heads for the key and query vector makes it possible to have two attention weights.</p>
<ul>
<li>
<p>The attention scores the same as in self-attention with scaling and a causal masked applied.</p>
<pre tabindex="0"><code>q *= self.scaling

q = q.transpose(1,2)
k = k.transpose(1,2)
v = v.transpose(1,2)

attn_weights = torch.matmul(q, k.transpose(2,3))
if attn_mask is None:
    attn_mask = torch.triu(
        torch.zeros((tgt_len, tgt_len)
        ).float()
        .type_as(attn_weights),
        diagonal= 1+offset
    )

attn_weights = torch.nan_to_num(attn_weights)
attn_weights += attn_mask
</code></pre></li>
<li>
<p>Softmax is applied to get the attention weights.</p>
<pre tabindex="0"><code>attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).type_as(
        attn_weights
    )
</code></pre></li>
<li>
<p>\( \lambda \) is calculated as :</p>
<pre tabindex="0"><code>lambda_1 = torch.exp(
        torch.sum(
            self.lambda_q1 * self.lambda_k1, dim=-1).float()).type_as(q)
lambda_2 = torch.exp(
    torch.sum(
        self.lambda_q2 * self.lambda_k2, dim=-1).float()).type_as(q) + self.lambda_init
lambda_full = lambda_1 - lambda_2
</code></pre></li>
</ul>
<p>where lambda_1 and lambda_2 are the LHS and RHS of the equation we saw earlier :</p>
\[
\lambda = \exp(\lambda_{q1} \cdot \lambda_{k1}) - \exp(\lambda_{q2} \cdot \lambda_{k2}) + \lambda_{\text{init}}
\]
<ul>
<li>
<p>Here&rsquo;s the key part, the attention weights are now halved across the head_dimensions, so the model now has half the number of heads, and two different attention weights in each head.</p>
<pre tabindex="0"><code>attn_weights = attn_weights.view(
    bsz, 
    self.num_heads,
    2, tgt_len, src_len)
</code></pre></li>
<li>
<p>The difference between both attention weights is then:</p>
</li>
</ul>
<pre tabindex="0"><code>attn_weights = attn_weights[:, :, 0] - lambda_full * attn_weights[:, :, 1]
</code></pre><ul>
<li>
<p>Finally the attention weights are multiplied with the value vector to create the new context vectors, RMS norm is applied and the heads are concatenated into one output vector that has dim = embed_dim</p>
<pre tabindex="0"><code>ctx_vec= torch.matmul(attn_weights, v)
ctx_vec = self.sublayer_norm(ctx_vec)
ctx_vec = ctx_vec.transpose(1,2).reshape(
    bsz, tgt_len, 
    self.num_heads * 2 * self.head_dim)
</code></pre></li>
</ul>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>Differential transformers have demonstrated great potential early on, achieving comparable performances at 65% the size of transformers in language modelling and outpeform transformers on various downstream tasks.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="//localhost:1313/tags/natural-language-processing/">Natural Language Processing</a></li>
      <li><a href="//localhost:1313/tags/transformers/">Transformers</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="//localhost:1313/articles/text-desrambler/">
    <span class="title">Next »</span>
    <br>
    <span>Finetuning GPT2 to Reconstruct Sentences</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>Copyright Damilola John © 2023</span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
