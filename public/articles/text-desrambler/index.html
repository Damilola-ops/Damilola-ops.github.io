<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Finetuning GPT2 to Reconstruct Sentences | Home</title>
<meta name="keywords" content="Natural Language Processing, Transformers">
<meta name="description" content="Two words are anagrams if one can be formed by permuting the letters of the other. Applying the same logic to a sentence, would be saying that two sentences are anagrams(no such thing) if their component words can be permutated to form clones of each other.
I thought it would be interesting to teach a language model to do this. You might be thinking that simply re-arranging words in a sentence doesn&rsquo;t require intelligence and can be done with very trivial algorithms,you would be right, but I added an edge to this task, given a random sequence of words, the language model has to return a grammatically correct sequence using the same set of words.">
<meta name="author" content="Damilola John">
<link rel="canonical" href="//localhost:1313/articles/text-desrambler/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.bf5f9f73cf17311d52cedbcda82c922e91b2f566d88a85ad9f5b5a08b586bd5f.css" integrity="sha256-v1&#43;fc88XMR1SztvNqCySLpGy9WbYioWtn1taCLWGvV8=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js" integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="//localhost:1313/static/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="//localhost:1313/static/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="//localhost:1313/static/favicon-32x32.png">
<link rel="apple-touch-icon" href="//localhost:1313/static/apple-touch-icon.png">
<link rel="mask-icon" href="//localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="//localhost:1313/articles/text-desrambler/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Finetuning GPT2 to Reconstruct Sentences" />
<meta property="og:description" content="Two words are anagrams if one can be formed by permuting the letters of the other. Applying the same logic to a sentence, would be saying that two sentences are anagrams(no such thing) if their component words can be permutated to form clones of each other.
I thought it would be interesting to teach a language model to do this. You might be thinking that simply re-arranging words in a sentence doesn&rsquo;t require intelligence and can be done with very trivial algorithms,you would be right, but I added an edge to this task, given a random sequence of words, the language model has to return a grammatically correct sequence using the same set of words." />
<meta property="og:type" content="article" />
<meta property="og:url" content="//localhost:1313/articles/text-desrambler/" />
<meta property="og:image" content="//localhost:1313/anagrams.jpg" /><meta property="article:section" content="articles" />
<meta property="article:published_time" content="2024-06-15T04:14:46+01:00" />
<meta property="article:modified_time" content="2024-06-15T04:14:46+01:00" /><meta property="og:site_name" content="Damilola John" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="//localhost:1313/anagrams.jpg" />
<meta name="twitter:title" content="Finetuning GPT2 to Reconstruct Sentences"/>
<meta name="twitter:description" content="Two words are anagrams if one can be formed by permuting the letters of the other. Applying the same logic to a sentence, would be saying that two sentences are anagrams(no such thing) if their component words can be permutated to form clones of each other.
I thought it would be interesting to teach a language model to do this. You might be thinking that simply re-arranging words in a sentence doesn&rsquo;t require intelligence and can be done with very trivial algorithms,you would be right, but I added an edge to this task, given a random sequence of words, the language model has to return a grammatically correct sequence using the same set of words."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Articles",
      "item": "//localhost:1313/articles/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Finetuning GPT2 to Reconstruct Sentences",
      "item": "//localhost:1313/articles/text-desrambler/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Finetuning GPT2 to Reconstruct Sentences",
  "name": "Finetuning GPT2 to Reconstruct Sentences",
  "description": "Two words are anagrams if one can be formed by permuting the letters of the other. Applying the same logic to a sentence, would be saying that two sentences are anagrams(no such thing) if their component words can be permutated to form clones of each other.\nI thought it would be interesting to teach a language model to do this. You might be thinking that simply re-arranging words in a sentence doesn\u0026rsquo;t require intelligence and can be done with very trivial algorithms,you would be right, but I added an edge to this task, given a random sequence of words, the language model has to return a grammatically correct sequence using the same set of words.",
  "keywords": [
    "Natural Language Processing", "Transformers"
  ],
  "articleBody": "Two words are anagrams if one can be formed by permuting the letters of the other. Applying the same logic to a sentence, would be saying that two sentences are anagrams(no such thing) if their component words can be permutated to form clones of each other.\nI thought it would be interesting to teach a language model to do this. You might be thinking that simply re-arranging words in a sentence doesn’t require intelligence and can be done with very trivial algorithms,you would be right, but I added an edge to this task, given a random sequence of words, the language model has to return a grammatically correct sequence using the same set of words. For example, the following sequence:\nThe equations expensive. show is optimization computationally that should return:\nThe equations show that optimization is computationally expensive. More examples:\n'the which wiring flow. propose to diagram, method network a reflects signal We visualize' ---\u003e 'We propose a method to visualize the wiring diagram, which reflects network signal flow.'\r,\r'the interaction networks. the gap Finally, analyze chemical the junction between synapse and we', ---\u003e 'Finally, we analyze the interaction between the gap junction and the chemical synapse networks.'\r'the process The pseudorandom number illustrated in is Mathematica. generator using' ---\u003e 'The process is illustrated using the pseudorandom number generator in Mathematica.',\r'statistical estimators functionals. of various of consistent We investigate existence the bounded-memory' ---\u003e 'We investigate the existence of bounded-memory consistent estimators of various statistical functionals.'\r'rather negative sense. the question This in strong a is in resolved' ---\u003e 'This question is resolved in the negative in a rather strong sense.' In this article, I would be explaining how I fine-tuned a small version of GPT-2 (334m parameters) on the task of turning descrambled sentences into their grammatically correct forms using the same words in the sentence. I would be exploring the different challenges and trade-offs that were made faced from data preparation and training optimizations, all the way to text-generation strategies\nData The dataset consists of 40000 rows of descrambled sentences and their labels, with columns [’text’,‘id’,’label’].\nBuilding the dataset involved scraping wikipedia pages and picking random sentences from them. This sentences were then permutted to create the ’text’ - ’label’ pairs used for training the language model. To ensure a reasonable data distribution, I scraped pages ranging from niche Medicine, engineering and mathematics pages to literature and even fandom pages.\nDatasetDict({\rtrain: Dataset({\rfeatures: ['text', 'label', 'id'],\rnum_rows: 40001\r})\rtest: Dataset({\rfeatures: ['text', 'label', 'id'],\rnum_rows: 10000\r})\rval: Dataset({\rfeatures: ['text', 'label', 'id'],\rnum_rows: 4001\r})\r}) The dataset consists of 54000 rows of sentences, split into train, test and validation.\nData Preprocessing One of the key challenges in finetuning GPT-2 for sentence reconstruction was in framing the problem in way that was learnable for the model. As a decoder-only model with causal mask attention, GPT-2 is trained to predict the next token using only the previous tokens in the sequence. This is fundamentally different from the typical sequence-to-sequence (seq2seq) approach, where the entire input sentence is used as context to predict each token in the target sentence.\nIn a standard sequence-to-sequence task, an encoder-decoder model like the one used in the original Transformer paper is the reasonable choice. The encoder processes the input sentence, and the decoder generates the output sentence by attending to the encoder’s representations. This allows the model to directly leverage the input context when producing the target sequence.\nHowever, with a decoder-only model like GPT-2, we need to rethink how we structure the dataset and the training process. The problem here lies in ensuring gpt2 ‘sees’ the permutted form of the sentence and uses this to generate the output, this way, we can hope our model learns to use the previous incorrect sentence to generate a grammatically meaningful anagram of the input.\nEach sentence in the training dataset was formatted to the following template:\nWrong sentence: {scrambled sentence}\rCorrect sentence: {descrambled sentence} By providing the model with the full scrambled sentence and asking it to complete the task by outputting the correct, unscrambled version, we have essentially framed a sequence-sequence task as a text generation one.\ndef preprocess_data(row):\rtarget_text = row['label']\r# add prompt to every row in the dataset\rinput_text = f'''wrong sentence: {row['text']} correct sentence:'''\r# find the length of the input prompt prompt_len = len(tokenizer(input_text).input_ids)\rinput = tokenizer(f'{input_text} {target_text} \u003c|endoftext|\u003e',\rpadding='max_length', truncation=True, max_length=128, return_tensors='pt').to(device)\rinput_ids, attention_mask = input.input_ids, input.attention_mask\r# turn all of the tokens before the actual correct sentence to -100\r# so loss is only calculated for generation after 'correct sentence:'\rlabels = input_ids.clone()\rlabels[:, :prompt_len] = -100\r# Turn all pad tokens to -100\rlabels[labels == tokenizer.pad_token_id] = -100\rassert (labels == -100).sum() \u003e len(labels), \"Labels are all -100,something is wrong.\"\r# if (labels == -100).sum() == len(labels) - 1:\r# raise\rreturn {'input_ids': input_ids.squeeze(),\r'attention_mask': attention_mask.squeeze(), 'labels': labels.squeeze(),\r'prompt': input_text}\rprocessed_data = dataset.map(preprocess_data)\rprocessed_data.set_format(type='torch', columns=['input_ids',\r'attention_mask', 'labels'\r])\rprocessed_data = dataset.map(preprocess_data,batched=True,batch_size=256)\rprocessed_data.set_format(type='torch',columns=['input_ids','attention_mask','labels']) The Preprocess Function The first three lines of the function creates our prompt template and tokenizes every row in our dataset. Next we calculate the length of the prompt part of the template i.e \" wrong sentence: {input text} correct sentence:\" . We then concatenate the input and labels together to form our template and successfully turn our problem to a next word prediction problem.\nThen set every token in the labels before ‘correct sentence:’ to -100 (Transformer’s library doesn’t calculate loss for this token and was chosen arbitrarily). Why ?\nThe reason for doing this is to ensure that the model doesn’t waste steps learning how to predict the prompt part of the sentence and only learns to predict tokens that comes after the ‘correct sentence:’. This was done to make sure that we are backpropagating losses related to descrambling sentences only and not add any noise that might confuse the model further\nThis is done by ensuring the model only calculates loss on the actual tokens we need to learn. -100 is the ignore index by pytorch’s cross_entropy_loss function and was chosen arbitrarily.\nFinally the preprocessing is completed by also setting all pad tokens in the labels to -100 for the reason stated above, we don’t want our model paying attention to padding tokens.\nHere’s a visual breakdown of the whole process Sentence =\nimportant and unmixing data challenging an problem hyperspectral Spectral in processing. is –\u003e Tokenizer –\u003e input_ids :\nTensor([36460, 6827, 25, 286, 318, 31760, 7468, 10393, 317, 2276,\r5545, 13, 3376, 6827, 25, 31760, 7468, 286, 317, 10393,\r318, 5545, 13, 2276, 220, 50256, 50256, 50256, 50256, 50256,\r50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\r50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\r50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\r50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\r50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\r50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\r50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\r50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\r50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\r50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]) with 50256 being the pad_token_ids.\ninput_ids —\u003e Preprocess function –\u003e\ntensor([ -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\r-100, -100, -100, -100, -100, 31760, 7468, 286, 317, 10393,\r318, 5545, 13, 2276, 220, -100, -100, -100, -100, -100,\r-100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\r-100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\r-100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\r-100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\r-100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\r-100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\r-100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\r-100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\r-100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\r-100, -100, -100, -100, -100, -100, -100, -100]) every token but the tokens corresponding to everything after ‘correct sentence’ or the descrambled sentence is converted to -100 in the label, ensuring that loss is only calculated on this tokens.\nHardware For this task, the Nvidia A10G(24GB) provided by modal was used for training, since GPT2-medium has 335m parameters and I trained in 16bit precision, adding up to about 5.6gb in memory to store model weights, and for training about twice that for training(optimizer states and gradients). Using a 16gb GPU with a reasonable batch size would have been sufficient, however training for shorter periods (larger batch sizes) on a more expensive GPU turned out to be cheaper.\ndevice = torch.device('cuda'if torch.cuda.is_available() else 'cpu') Finetuning Finetuning in modal was fairly straightforward from this point, all I had to do was create a docker-like image containing all of our needed libraries that runs in a sandboxed gvisor container and installed all our neccessary libraries.\nNext we wrote functions such as our trainer class and other utils to be executed in the container environment we created. For training, I used a learning rate of 3e-5 (5e-5 is typically used to train gpt2 but I tried a smaller learning rate since I used a larger batch size).\ntraining_args = TrainingArguments(\rper_device_train_batch_size=batch_size,\rnum_train_epochs=5,\rlearning_rate=3e-5,\routput_dir=str(VOL_MOUNT_PATH / \"model\"),\rlogging_dir=str(VOL_MOUNT_PATH / \"logs\"),\rlogging_strategy='steps',\rlogging_steps=100,\rload_best_model_at_end=True,\rsave_strategy='steps',\revaluation_strategy='steps',\rsave_steps=100,\rsave_total_limit=2,\rpush_to_hub=False,\rreport_to='wandb',\rfp16=True\r)\rtrainer = Trainer(\rmodel=model,\rargs=training_args,\rtrain_dataset=processed_data['train'],\reval_dataset=processed_data['validation'],\r)\rtrainer.train()\rwandb.finish()\rmodel.save_pretrained(str(VOL_MOUNT_PATH / \"model\"))\rtokenizer.save_pretrained(str(VOL_MOUNT_PATH / \"tokenizer\")) I trained for 5 epochs(actually the validation loss stopped improving after the 3rd epoch and I was a bit worried about overfitting as I didn’t set any early stopping criteria)\nOptimization Using some memory optimization training techniques for a single GPU like CPU offloading(moving optimizer states to main memory), gradient checkpointing and gradient accumulation, I was able to fit a batch size of 128 in memory. Training was also done in FP16.\nInference Now to the fun part, testing out the model.Testing the model with the same inputs displayed earlier in the article which were chosen randomly from the test set, Here are the outputs :\nOutput: wrong sentence: the which wiring flow. propose to diagram, method network a reflects signal We visualize correct sentence: ---\u003e The diagram, which reflects the wiring network to propose a signal flow. Output: wrong sentence: the interaction networks. the gap Finally, analyze chemical the junction between synapse and we correct sentence: ---\u003eThe gap junction and the chemical interaction between the synapse networks. Output: wrong sentence: the process The pseudorandom number illustrated in is Mathematica. generator using correct sentence:The pseudorandom number generator is illustrated using the process in Mathematica. Output: wrong sentence: in the of structure resulted decrease mutual signal in information. Introducing correlations input-output correct sentence: ---\u003e This resulted in decrease of mutual correlations in the input-output structure of signal information. Output: wrong sentence: statistical estimators functionals. of various of consistent We investigate existence the bounded-memory correct sentence: ---\u003e The existence of bounded-memory estimators consistent with various statistical functionals. Output: wrong sentence: rather negative sense. the question This in strong a is in resolved correct sentence: --\u003e The question is resolved in a rather strong negative sense. By merely visually inspecting the outputs the first thing I noticed was how easy it was to make up sentences that looked correct but actually make no sense. For example:\ndifferential low-power The comparators. fully uses ADC clocked our finetuned model output:\nOutput: wrong sentence: differential low-power The comparators. fully uses ADC clocked correct sentence:The ADC uses fully clocked low-power differential comparators. GPT3.5 :\nChatGPT’s output makes no sense whatsoever as there’s no such thing as a differential low power ADC\nChatgpt was prompted using some few-shot examples from the training set.\nGeneration Strategy For generation, I used beam-search as a decoding strategy as it felt like the most reasonable decoding strategy for this task since the sequence with the highest overall probability was more likely to be output I want.\nIn conclusion, the 335m parameter model finetuned on this task performs alot better than the 175b GPT3.5 with few-shot prompting. It was interesting to find that a 335m parameter model could learn to descramble sentences and generalize well on unseen samples.\nView the code here and the model on huggingface\n",
  "wordCount" : "2047",
  "inLanguage": "en",
  "image":"//localhost:1313/anagrams.jpg","datePublished": "2024-06-15T04:14:46+01:00",
  "dateModified": "2024-06-15T04:14:46+01:00",
  "author":{
    "@type": "Person",
    "name": "Damilola John"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "//localhost:1313/articles/text-desrambler/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Home",
    "logo": {
      "@type": "ImageObject",
      "url": "//localhost:1313/static/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="//localhost:1313/" accesskey="h" title="Home (Alt + H)">Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="//localhost:1313/about" title="About Me">
                    <span>About Me</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/articles/" title="Articles">
                    <span>Articles</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="//localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="//localhost:1313/articles/">Articles</a></div>
    <h1 class="post-title">
      Finetuning GPT2 to Reconstruct Sentences
    </h1>
    <div class="post-meta">&lt;span title=&#39;2024-06-15 04:14:46 &#43;0100 WAT&#39;&gt;June 15, 2024&lt;/span&gt;&amp;nbsp;·&amp;nbsp;10 min&amp;nbsp;·&amp;nbsp;2047 words&amp;nbsp;·&amp;nbsp;Damilola John

</div>
  </header> 
<figure class="entry-cover"><img loading="lazy" src="//localhost:1313/anagrams.jpg" alt="">
        
</figure><div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#data" aria-label="Data">Data</a></li>
                <li>
                    <a href="#data-preprocessing" aria-label="Data Preprocessing">Data Preprocessing</a></li>
                <li>
                    <a href="#the-preprocess-function" aria-label="The Preprocess Function">The Preprocess Function</a></li>
                <li>
                    <a href="#hardware" aria-label="Hardware">Hardware</a></li>
                <li>
                    <a href="#finetuning" aria-label="Finetuning">Finetuning</a><ul>
                        
                <li>
                    <a href="#optimization" aria-label="Optimization">Optimization</a></li></ul>
                </li>
                <li>
                    <a href="#inference" aria-label="Inference">Inference</a></li>
                <li>
                    <a href="#generation-strategy" aria-label="Generation Strategy">Generation Strategy</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Two words are anagrams if one can be formed by permuting the letters of the other. Applying the same logic to a sentence, would be saying that two sentences are anagrams(no such thing) if their component words can be permutated to form clones of each other.</p>
<p>I thought it would be interesting to teach a language model to do this. You might be thinking that simply re-arranging words in a sentence doesn&rsquo;t require intelligence and can be done with very trivial algorithms,you would be right,  but I added an edge to this task, given a random sequence of words, the language model has to return a grammatically correct sequence using the same set of words. For example, the following sequence:</p>
<pre tabindex="0"><code>The equations expensive. show is optimization computationally that
</code></pre><p>should return:</p>
<pre tabindex="0"><code>The equations show that optimization is computationally expensive.
</code></pre><p><img loading="lazy" src="https://images.aicrowd.com/uploads/ckeditor/pictures/457/content_giphy__8_.gif" alt="guy from the office"  />
</p>
<p>More examples:</p>
<pre tabindex="0"><code>&#39;the which wiring flow. propose to diagram, method network a reflects signal We visualize&#39;  ---&gt; &#39;We propose a method to visualize the wiring diagram, which reflects network signal flow.&#39;

,
&#39;the interaction networks. the gap Finally, analyze chemical the junction between synapse and we&#39;, ---&gt;  &#39;Finally, we analyze the interaction between the gap junction and the chemical synapse networks.&#39;


&#39;the process The pseudorandom number illustrated in is Mathematica. generator using&#39; ---&gt; &#39;The process is illustrated using the pseudorandom number generator in Mathematica.&#39;,


&#39;statistical estimators functionals. of various of consistent We investigate existence the bounded-memory&#39; ---&gt;  &#39;We investigate the existence of bounded-memory consistent estimators of various statistical functionals.&#39;


&#39;rather negative sense. the question This in strong a is in resolved&#39; ---&gt; &#39;This question is resolved in the negative in a rather strong sense.&#39;
</code></pre><p>In this article, I would be explaining  how I fine-tuned a small version of GPT-2 (334m parameters) on the task of turning descrambled sentences into their grammatically correct forms using the same words in the sentence. I would be exploring the different challenges and trade-offs that were made faced from data preparation and training optimizations, all the way to text-generation strategies</p>
<h2 id="data">Data<a hidden class="anchor" aria-hidden="true" href="#data">#</a></h2>
<p>The dataset consists of 40000 rows of descrambled sentences and their labels, with columns [&rsquo;text&rsquo;,&lsquo;id&rsquo;,&rsquo;label&rsquo;].</p>
<p>Building the dataset involved scraping wikipedia pages and picking random sentences from them. This sentences were then permutted to create the &rsquo;text&rsquo; - &rsquo;label&rsquo; pairs used for training the language model. To ensure a reasonable data distribution, I scraped pages ranging from niche Medicine, engineering and mathematics pages to literature and even fandom pages.</p>
<pre tabindex="0"><code>DatasetDict({
    train: Dataset({
        features: [&#39;text&#39;, &#39;label&#39;, &#39;id&#39;],
        num_rows: 40001
    })
    test: Dataset({
        features: [&#39;text&#39;, &#39;label&#39;, &#39;id&#39;],
        num_rows: 10000
    })
    val: Dataset({
        features: [&#39;text&#39;, &#39;label&#39;, &#39;id&#39;],
        num_rows: 4001
    })
})
</code></pre><p>The dataset consists of 54000 rows of sentences, split into train, test and validation.</p>
<h2 id="data-preprocessing">Data Preprocessing<a hidden class="anchor" aria-hidden="true" href="#data-preprocessing">#</a></h2>
<p>One of the key challenges in finetuning GPT-2 for sentence reconstruction was in framing the problem in way that was learnable for the model. As a decoder-only model with causal mask attention, GPT-2 is trained to predict the next token using only the previous tokens in the sequence. This is fundamentally different from the typical sequence-to-sequence (seq2seq) approach, where the entire input sentence is used as context to predict each token in the target sentence.</p>
<p>In a standard sequence-to-sequence task, an encoder-decoder model like the one used in the original Transformer paper is the reasonable choice. The encoder processes the input sentence, and the decoder generates the output sentence by attending to the encoder&rsquo;s representations. This allows the model to directly leverage the input context when producing the target sequence.</p>
<p>However, with a decoder-only model like GPT-2, we need to rethink how we structure the dataset and the training process. The problem here lies in ensuring gpt2 &lsquo;sees&rsquo; the permutted form of the sentence and uses this to generate the output, this way, we can hope our model learns to use the previous incorrect sentence to generate a grammatically meaningful anagram of the input.</p>
<p>Each sentence in the  training dataset was formatted to the following template:</p>
<pre tabindex="0"><code>Wrong sentence: {scrambled sentence}
Correct sentence: {descrambled sentence}
</code></pre><p>By providing the model with the full scrambled sentence and asking it to complete the task by outputting the correct, unscrambled version, we have essentially framed a sequence-sequence task as a text generation one.</p>
<pre tabindex="0"><code>def preprocess_data(row):
        target_text = row[&#39;label&#39;]
        # add prompt to every row in the dataset
        input_text = f&#39;&#39;&#39;wrong sentence: {row[&#39;text&#39;]} correct sentence:&#39;&#39;&#39;
        # find the length of the input prompt 
        prompt_len = len(tokenizer(input_text).input_ids)
        input = tokenizer(f&#39;{input_text} {target_text} &lt;|endoftext|&gt;&#39;,
                          padding=&#39;max_length&#39;, truncation=True, 
                          max_length=128, return_tensors=&#39;pt&#39;).to(device)
        input_ids, attention_mask = input.input_ids, input.attention_mask
        # turn all of the tokens before the actual correct sentence to -100
        # so loss is only calculated for generation after &#39;correct sentence:&#39;
        labels = input_ids.clone()
        labels[:, :prompt_len] = -100
        # Turn all pad tokens to -100
        labels[labels == tokenizer.pad_token_id] = -100
        assert (labels == -100).sum() &gt; len(labels), &#34;Labels are all -100,something is wrong.&#34;
        # if (labels == -100).sum() == len(labels) - 1:
        #         raise
        return {&#39;input_ids&#39;: input_ids.squeeze(),
                &#39;attention_mask&#39;: attention_mask.squeeze(), 
                &#39;labels&#39;: labels.squeeze(),
                &#39;prompt&#39;: input_text}
    processed_data = dataset.map(preprocess_data)
    processed_data.set_format(type=&#39;torch&#39;, columns=[&#39;input_ids&#39;,
                                                     &#39;attention_mask&#39;, &#39;labels&#39;
                                                     ])



processed_data = dataset.map(preprocess_data,batched=True,batch_size=256)
processed_data.set_format(type=&#39;torch&#39;,columns=[&#39;input_ids&#39;,&#39;attention_mask&#39;,&#39;labels&#39;])
</code></pre><h2 id="the-preprocess-function">The Preprocess Function<a hidden class="anchor" aria-hidden="true" href="#the-preprocess-function">#</a></h2>
<p>The first three lines of the function creates our prompt template and tokenizes every row in our dataset. Next we calculate the length of the prompt part of the template i.e &quot; wrong sentence: {input text} correct sentence:&quot; . We then concatenate the input and labels together to form our template and successfully turn our problem to a next word prediction problem.</p>
<p>Then set every token in the labels before &lsquo;correct sentence:&rsquo; to -100 (Transformer&rsquo;s library doesn&rsquo;t calculate loss for this token and was chosen arbitrarily).
Why ?</p>
<p>The reason for doing this is to ensure that the model doesn&rsquo;t waste  steps learning how to predict the prompt part of the sentence and only learns to predict tokens that comes after the  &lsquo;correct sentence:&rsquo;. This was done to make sure that we are  backpropagating losses related to descrambling sentences only and not add any noise that might confuse the model further</p>
<p>This is done by ensuring the model only calculates loss on the actual tokens we need to learn. -100 is  the ignore index by pytorch&rsquo;s cross_entropy_loss function and was chosen arbitrarily.</p>
<p>Finally the preprocessing is completed by also setting all pad tokens in the labels to -100 for the reason stated above, we don&rsquo;t want our model paying attention to padding tokens.</p>
<p>Here&rsquo;s a visual breakdown of the whole process
Sentence =</p>
<pre tabindex="0"><code></code></pre><p><code>important and unmixing data challenging an problem hyperspectral Spectral in processing. is</code> &ndash;&gt; Tokenizer &ndash;&gt; input_ids :</p>
<pre tabindex="0"><code>Tensor([36460,  6827,    25,   286,   318, 31760,  7468, 10393,   317,  2276,
         5545,    13,  3376,  6827,    25, 31760,  7468,   286,   317, 10393,
          318,  5545,    13,  2276,   220, 50256, 50256, 50256, 50256, 50256,
        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,
        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])
</code></pre><p>with 50256 being the pad_token_ids.</p>
<p>input_ids  &mdash;&gt; Preprocess function &ndash;&gt;</p>
<pre tabindex="0"><code>tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100, 31760,  7468,   286,   317, 10393,
          318,  5545,    13,  2276,   220,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])
</code></pre><p>every token but the tokens corresponding to everything after &lsquo;correct sentence&rsquo; or the descrambled sentence is converted to -100 in the label, ensuring that loss is only calculated on this tokens.</p>
<h2 id="hardware">Hardware<a hidden class="anchor" aria-hidden="true" href="#hardware">#</a></h2>
<p>For this task, the Nvidia A10G(24GB) provided by <a href="https://modal.com">modal</a> was used for training, since GPT2-medium has 335m parameters and I trained in 16bit precision, adding up to about 5.6gb in memory to store model weights, and for training about twice that for training(optimizer states and gradients). Using a 16gb GPU with a reasonable batch size would have been sufficient, however training for shorter periods (larger batch sizes) on a  more expensive GPU turned out to be cheaper.</p>
<pre tabindex="0"><code>device = torch.device(&#39;cuda&#39;if torch.cuda.is_available() else &#39;cpu&#39;)
</code></pre><h2 id="finetuning">Finetuning<a hidden class="anchor" aria-hidden="true" href="#finetuning">#</a></h2>
<p>Finetuning in modal was fairly straightforward from this point, all I had to do was create a docker-like image containing all of our needed libraries that runs in a sandboxed <a href="https://cloud.google.com/blog/products/identity-security/open-sourcing-gvisor-a-sandboxed-container-runtime">gvisor container</a> and installed all our neccessary libraries.</p>
<p>Next we wrote functions such as our trainer class and other utils to be executed in the container environment we created.
For training, I used a learning rate of 3e-5 (5e-5 is typically used to train gpt2 but I tried a smaller learning rate since I used a larger batch size).</p>
<pre tabindex="0"><code> training_args = TrainingArguments(
        per_device_train_batch_size=batch_size,
        num_train_epochs=5,
        learning_rate=3e-5,
        output_dir=str(VOL_MOUNT_PATH / &#34;model&#34;),
        logging_dir=str(VOL_MOUNT_PATH / &#34;logs&#34;),
        logging_strategy=&#39;steps&#39;,
        logging_steps=100,
        load_best_model_at_end=True,
        save_strategy=&#39;steps&#39;,
        evaluation_strategy=&#39;steps&#39;,
        save_steps=100,
        save_total_limit=2,
        push_to_hub=False,
        report_to=&#39;wandb&#39;,
        fp16=True
        )
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=processed_data[&#39;train&#39;],
        eval_dataset=processed_data[&#39;validation&#39;],
        )
    trainer.train()
    wandb.finish()
    model.save_pretrained(str(VOL_MOUNT_PATH / &#34;model&#34;))
    tokenizer.save_pretrained(str(VOL_MOUNT_PATH / &#34;tokenizer&#34;))
</code></pre><p>I trained for 5 epochs(actually the validation loss stopped improving after the 3rd epoch and I was a bit worried about overfitting as I didn&rsquo;t set any early stopping criteria)</p>
<p><img loading="lazy" src="https://proglangclassifier.s3.eu-west-2.amazonaws.com/text-descrambler-train_loss.png" alt="train loss"  />

<img loading="lazy" src="https://proglangclassifier.s3.eu-west-2.amazonaws.com/eval_loss_text_Descrambler.png" alt="validation loss"  />
</p>
<h3 id="optimization">Optimization<a hidden class="anchor" aria-hidden="true" href="#optimization">#</a></h3>
<p>Using some memory optimization training techniques for a single GPU like CPU offloading(moving optimizer states to main memory), gradient checkpointing and gradient accumulation, I was able to fit a batch size of 128 in memory. Training was also done in FP16.</p>
<h2 id="inference">Inference<a hidden class="anchor" aria-hidden="true" href="#inference">#</a></h2>
<p>Now to the fun part, testing out the model.Testing the model with the same inputs displayed earlier in the article which were chosen randomly from the test set, Here are the outputs :</p>
<pre tabindex="0"><code>Output: wrong sentence: the which wiring flow. propose to diagram, method network a reflects signal We visualize correct sentence: ---&gt; The diagram, which reflects the wiring network to propose a signal flow.                                                                                                                  
Output: wrong sentence: the interaction networks. the gap Finally, analyze chemical the junction between synapse and we correct sentence: ---&gt;The gap junction and the chemical interaction between the synapse networks.                                                                                                                   
Output: wrong sentence: the process The pseudorandom number illustrated in is Mathematica. generator using correct sentence:The pseudorandom number generator is illustrated using the process in Mathematica.                                                                                                                
Output: wrong sentence: in the of structure resulted decrease mutual signal in information. Introducing correlations input-output correct sentence: ---&gt; This resulted in decrease of mutual correlations in the input-output structure of signal information.                                                                                                               
Output: wrong sentence: statistical estimators functionals. of various of consistent We investigate existence the bounded-memory correct sentence: ---&gt; The existence of bounded-memory estimators consistent with various statistical functionals.                                                                                                                 
Output: wrong sentence: rather negative sense. the question This in strong a is in resolved correct sentence: --&gt; The question is resolved in a rather strong negative sense.
</code></pre><p>By merely visually inspecting the outputs the first thing I noticed was how easy it was to make up sentences that looked correct but actually make no sense. For example:</p>
<pre tabindex="0"><code>differential low-power The comparators. fully uses ADC clocked
</code></pre><p>our finetuned model output:</p>
<pre tabindex="0"><code>Output: wrong sentence: differential low-power The comparators. fully uses ADC clocked correct sentence:The ADC uses fully clocked low-power differential comparators.
</code></pre><p>GPT3.5 :</p>
<p><img loading="lazy" src="https://proglangclassifier.s3.eu-west-2.amazonaws.com/Screenshot&#43;%2817%29.png" alt="chatgpt&amp;rsquo;s answer"  />
</p>
<p>ChatGPT&rsquo;s output makes no sense whatsoever as there&rsquo;s no such thing as a differential low power ADC</p>
<p><strong>Chatgpt was prompted using some few-shot examples from the training set.</strong></p>
<h2 id="generation-strategy">Generation Strategy<a hidden class="anchor" aria-hidden="true" href="#generation-strategy">#</a></h2>
<p>For generation, I used beam-search as a decoding strategy as it felt like the most reasonable decoding strategy for this task since the sequence with the highest overall probability was more likely to be output I want.</p>
<p>In conclusion, the 335m parameter model finetuned on this task performs alot better than the 175b GPT3.5 with few-shot prompting. It was interesting to find that a 335m parameter model could learn to descramble sentences and generalize well on unseen samples.</p>
<p>View the code <a href="https://github.com/damilojohn/Text-Descrambling">here</a>
and the model on <a href="">huggingface</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="//localhost:1313/tags/natural-language-processing/">Natural Language Processing</a></li>
      <li><a href="//localhost:1313/tags/transformers/">Transformers</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="//localhost:1313/articles/language_classification/">
    <span class="title">Next »</span>
    <br>
    <span>Training BERT to identify 15 programming languages. </span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>Copyright Damilola John © 2023</span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
