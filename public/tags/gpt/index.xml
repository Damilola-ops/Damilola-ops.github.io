<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>GPT on Home</title>
    <link>//localhost:1313/tags/gpt/</link>
    <description>Recent content in GPT on Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright Damilola John &amp;copy; 2023</copyright>
    <lastBuildDate>Tue, 15 Aug 2023 04:14:46 +0100</lastBuildDate><atom:link href="//localhost:1313/tags/gpt/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Finetuning GPT2 to Descramble Sentences</title>
      <link>//localhost:1313/posts/text-desrambler/</link>
      <pubDate>Tue, 15 Aug 2023 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/posts/text-desrambler/</guid>
      <description>Goal In this article, we would be fine-tuning OPENAI&amp;rsquo;s GPT2 on a non-trivial task of turning descrambled sentences into their grammatically correct forms using the same words in the sentence.
This essentially means that we would be teaching GPT2 how to turn sentences from:
The equations expensive. show is optimization computationally that to :
The equations show that optimization is computationally expensive. More examples:
&amp;#39;the which wiring flow. propose to diagram, method network a reflects signal We visualize&amp;#39;,&amp;#39;the interaction networks.</description>
    </item>
    
  </channel>
</rss>
