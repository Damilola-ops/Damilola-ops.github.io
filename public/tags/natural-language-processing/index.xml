<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Natural Language Processing on Home</title>
    <link>//localhost:1313/tags/natural-language-processing/</link>
    <description>Recent content in Natural Language Processing on Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright Damilola John &amp;copy; 2023</copyright>
    <lastBuildDate>Sat, 19 Aug 2023 04:14:46 +0100</lastBuildDate><atom:link href="//localhost:1313/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Teaching BERT to identify 15 programming languages. </title>
      <link>//localhost:1313/posts/language_classification/</link>
      <pubDate>Sat, 19 Aug 2023 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/posts/language_classification/</guid>
      <description>This is a fun side project where I tried to build a model that could identify 15 of the most popular programming languages. We would start with simple machine learning approaches and gradually work our way up to more complex methods till we have a satisfactory solution.
The Dataset Our dataset is a csv containing 45,000 samples. The dataset is made up of two columns, the &amp;lsquo;code&amp;rsquo; feature contains code snippets we want to classify and the language column, which is our label contains the programming language it belongs to.</description>
    </item>
    
    <item>
      <title>Finetuning GPT2 to Descramble Sentences</title>
      <link>//localhost:1313/posts/text-desrambler/</link>
      <pubDate>Tue, 15 Aug 2023 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/posts/text-desrambler/</guid>
      <description>Goal In this article, we would be fine-tuning OPENAI&amp;rsquo;s GPT2 on a non-trivial task of turning descrambled sentences into their grammatically correct forms using the same words in the sentence.
This essentially means that we would be teaching GPT2 how to turn sentences from:
The equations expensive. show is optimization computationally that to :
The equations show that optimization is computationally expensive. More examples:
&amp;#39;the which wiring flow. propose to diagram, method network a reflects signal We visualize&amp;#39;,&amp;#39;the interaction networks.</description>
    </item>
    
    <item>
      <title>Byte-Pair Encoding, The Tokenization algorithm powering Large LanguageÂ Models. </title>
      <link>//localhost:1313/posts/bpe/</link>
      <pubDate>Thu, 20 Jul 2023 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/posts/bpe/</guid>
      <description>Tokenization is an umbrella term for the methods used to turn texts into chunks of words or sub-words. Tokenization has a lot of applications in computer science, from compilers to Natural Language Processing. In this article, we would be focusing on tokenizers in Language models, in particular, a method of tokenization called Byte Pair Encoding. The last few years have witnessed a revolution in NLP catalyzed mainly by the introduction of the transformers architecture in 2017 with the paper &amp;lsquo;Attention is all you need &amp;rsquo; epitomized by the introduction of ChatGPT in late 2022.</description>
    </item>
    
  </channel>
</rss>
