<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Transformers on Home</title>
    <link>//localhost:1313/tags/transformers/</link>
    <description>Recent content in Transformers on Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright Damilola John &amp;copy; 2023</copyright>
    <lastBuildDate>Sat, 19 Aug 2023 04:14:46 +0100</lastBuildDate><atom:link href="//localhost:1313/tags/transformers/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>I Tried to teach BERT 15 programming languages. </title>
      <link>//localhost:1313/posts/language_classification/</link>
      <pubDate>Sat, 19 Aug 2023 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/posts/language_classification/</guid>
      <description>This is a fun side project where I tried to build a model that could identify 15 of the most popular programming languages. We would start with simple machine learning approaches and gradually work our way up to more complex methods till we have a satisfactory solution.
The Dataset Our dataset is a csv containing 45,000 samples. The dataset is made up of two columns, the &amp;lsquo;code&amp;rsquo; feature contains code snippets we want to classify and the language column, which is our label contains the programming language it belongs to.</description>
    </item>
    
    <item>
      <title>Learning to smell from Molecules </title>
      <link>//localhost:1313/posts/learning_to_smell/</link>
      <pubDate>Tue, 15 Aug 2023 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/posts/learning_to_smell/</guid>
      <description>The title of this post might be a little confusing but what we are really going to do is to try and build a model to classify smells from their molecular compounds. This little project intertwines realms of chemistry and machine learning and it doesn&amp;rsquo;t get more interesting than that.
This was definitely the most interesting machine learning thing I have done so far.</description>
    </item>
    
    <item>
      <title>Playlist Generator</title>
      <link>//localhost:1313/projects/playlist_generator/</link>
      <pubDate>Fri, 11 Aug 2023 02:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/projects/playlist_generator/</guid>
      <description>Playlist Generator An app that uses semantic search to generate afrobeat song playlists from input texts This is a side project where I built an end-to-end machine learning application that uses a sentence transformer model hosted as a model-as-a-service. It is a simple recommendation system that tries to find afrobeat songs about a user&amp;rsquo;s sentiment by comparing the user&amp;rsquo;s text inputs to song lyrics and then return the most similar songs.</description>
    </item>
    
    <item>
      <title>Byte-Pair Encoding, The Tokenization algorithm powering Large LanguageÂ Models. </title>
      <link>//localhost:1313/posts/bpe/</link>
      <pubDate>Thu, 20 Jul 2023 04:14:46 +0100</pubDate>
      
      <guid>//localhost:1313/posts/bpe/</guid>
      <description>Tokenization is an umbrella term for the methods used to turn texts into chunks of words or sub-words. Tokenization has a lot of applications in computer science, from compilers to Natural Language Processing. In this article, we would be focusing on tokenizers in Language models, in particular, a method of tokenization called Byte Pair Encoding. The last few years have witnessed a revolution in NLP catalyzed mainly by the introduction of the transformers architecture in 2017 with the paper &amp;lsquo;Attention is all you need &amp;rsquo; epitomized by the introduction of ChatGPT in late 2022.</description>
    </item>
    
  </channel>
</rss>
